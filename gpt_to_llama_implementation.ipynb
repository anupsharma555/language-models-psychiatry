{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0592ce",
   "metadata": {},
   "source": [
    "# GPT to Llama Implementation\n",
    "\n",
    "Author: Anup Sharma, MD, PhD\n",
    "\n",
    "Implementing the conversion from GPT architecture to Llama models using templates from \"Building a Large Language Model from Scratch\", META AI open-source Llama family of models, and Hugging Face Hub. \n",
    "\n",
    "Based on the context from the standalone Llama 3.2 notebook, this implementation includes the key architectural differences between GPT and Llama models, such as the transition from standard multi-head attention to Grouped Query Attention (GQA), the implementation of RoPE (Rotary Position Embedding) instead of absolute positional encodings, the use of SwiGLU activation functions in the feed-forward networks, and RMSNorm instead of LayerNorm for normalization.\n",
    "\n",
    "The implementation is an ongoing implementation of a practical guide for understanding how modern language models like Llama 3.2 evolved from the original GPT architecture, providing hands-on code examples of each component transformation. This would be particularly valuable for researchers and developers looking to understand the technical improvements that make Llama models more efficient and performant compared to earlier versions of transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce6bb4",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "\n",
    "### Overview\n",
    "Converting GPT architecture to Llama 3.2 with modern improvements: GQA, RoPE, SwiGLU, RMSNorm\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "#### 1. FeedForward (SwiGLU)\n",
    "- **Purpose**: Gated feed-forward network with SiLU activation\n",
    "- **Key**: `silu(fc1(x)) * fc2(x)` → fc3\n",
    "- **Improvement**: More efficient than ReLU-based FFN\n",
    "\n",
    "#### 2. RoPE Functions\n",
    "- **compute_rope_params()**: Precompute sin/cos for position encoding\n",
    "- **apply_rope()**: Apply rotary transformation to Q/K\n",
    "- **Improvement**: Relative vs absolute positional encoding\n",
    "\n",
    "#### 3. GroupedQueryAttention\n",
    "- **Purpose**: Memory-efficient attention mechanism\n",
    "- **Key**: Multiple query heads share K/V pairs\n",
    "- **Memory**: 32 heads, 8 KV groups → 4:1 query-to-KV ratio\n",
    "\n",
    "#### 4. TransformerBlock\n",
    "- **Components**: GQA + SwiGLU + RMSNorm + residuals\n",
    "- **Flow**: norm → attention → residual → norm → ffn → residual\n",
    "\n",
    "#### 5. Llama3Model\n",
    "- **Complete**: Embedding → 16 blocks → norm → output\n",
    "- **Buffers**: Precomputed RoPE values registered\n",
    "\n",
    "### Configuration (1B Model)\n",
    "```python\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary\n",
    "    \"context_length\": 131_072,  # Max sequence\n",
    "    \"emb_dim\": 2048,           # Embedding dim\n",
    "    \"n_heads\": 32,             # Attention heads\n",
    "    \"n_layers\": 16,            # Transformer layers\n",
    "    \"hidden_dim\": 8192,        # FFN intermediate\n",
    "    \"n_kv_groups\": 8,          # GQA groups\n",
    "    \"rope_base\": 500_000.0,    # RoPE theta\n",
    "    \"dtype\": torch.bfloat16,   # Memory efficiency\n",
    "    \"rope_freq\": {...}         # Frequency scaling\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e67ff3",
   "metadata": {},
   "source": [
    "### Packages needed for Llama implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d0e53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: blobfile in /Users/anup/Library/Python/3.10/lib/python/site-packages (3.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /Users/anup/Library/Python/3.10/lib/python/site-packages (from blobfile) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from blobfile) (2.2.2)\n",
      "Requirement already satisfied: lxml>=4.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from blobfile) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from blobfile) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.27.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: safetensors in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /Users/anup/Library/Python/3.10/lib/python/site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install packages individually for better error handling\n",
    "packages = [\n",
    "    \"blobfile\",\n",
    "    \"huggingface_hub\",\n",
    "    \"ipywidgets\",\n",
    "    \"safetensors\",\n",
    "    \"sentencepiece\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    %pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f4d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.1.0\n",
      "huggingface_hub version: 0.27.1\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         #required to download pretrained weights\n",
    "    \"huggingface_hub\",   #required to download pretrained weights\n",
    "    \"tiktoken\",          #required to implement the tokenizer\n",
    "    \"torch\",             #required to implement models\n",
    "]\n",
    "\n",
    "for packages in pkgs:\n",
    "    print(f\"{packages} version: {version(packages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682a6fd",
   "metadata": {},
   "source": [
    "### Architecture of Llama 3.2 including SwiGLU feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e022cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The architecture including a SwiGLU feed-forward network in LLama 3.2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE (Rotary Position Embedding) implementation to understand token relationships based on relative rather than absolute positions.\n",
    "\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Frequency adjustments\n",
    "    if freq_config is not None:\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # It's ok to use lower-precision after applying cos and sin rotation\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f310f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped Querty Attention (GQA): implementation by performing core attention mechanism computation and generating final output. GQA is a memory-efficient alternative to standard MHA where multiple query heads share the same key-value pairs, reducing computational overhead while maintaining performance.\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, num_heads,\n",
    "            num_kv_groups,\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # Reshape queries, keys, and values\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        # Transpose keys, values, and queries\n",
    "        keys = keys.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # Shape: (b, num_kv_groups, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # Apply RoPE\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "\n",
    "        # Expand keys and values to match the number of heads\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        # For example, before repeat_interleave along dim=1 (query groups):\n",
    "        #   [K1, K2]\n",
    "        # After repeat_interleave (each query group is repeated group_size times):\n",
    "        #   [K1, K1, K2, K2]\n",
    "        # If we used regular repeat instead of repeat_interleave, we'd get:\n",
    "        #   [K1, K2, K1, K2]\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        # Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c13218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code implements a complete Transformer block used in Llama models, combining all the key architectural improvements including Grouped Query Attention, SwiGLU feed-forward networks and RMSNorm normalization. It represents one complete layer of the LLama transformer stack, processing input through attention and feed-forward steps with proper residual connections.\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
    "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, mask, cos, sin)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A top-level class that combines all previously defined components (ex: GroupedQueryAttention, SwiGLU FeedForward, ROPE, RMSNorm) into a fully functional transformer model. It represents the entire neural network from token input to vocab predictions, incorporating all key architectural improvements that distinguish Llama from prior versions of GPT.\n",
    "\n",
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # Main model parameters\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.trf_blocks = nn.ModuleList(  # ModuleList since Sequential can only accept one input, and we need `x, mask, cos, sin`\n",
    "            [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5, dtype=cfg[\"dtype\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # Reusuable utilities\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=cfg[\"emb_dim\"] // cfg[\"n_heads\"],\n",
    "            theta_base=cfg[\"rope_base\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            freq_config=cfg[\"rope_freq\"]\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # Forward pass\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f31d6",
   "metadata": {},
   "source": [
    "## Initialization of the Llama 3.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ef8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Llama 3.2 1B\n",
    "\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,          # Vocabulary size\n",
    "    \"context_length\": 131_072,      # Context length to train\n",
    "    \"emb_dim\": 2048,                # Embedding dimension\n",
    "    \"n_heads\": 32,                  # Number of attention heads\n",
    "    \"n_layers\": 16,                 # Number of layers\n",
    "    \"hidden_dim\": 8192,             # Size of intermediate dim\n",
    "    \"n_kv_groups\": 8,               # Key-Value groups for GQA\n",
    "    \"rope_base\": 500_000.0,          # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,        # dytpe for memory usage\n",
    "    \"rope_freq\": {\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Llama 3.2  model config information\n",
    "\n",
    "# LLAMA32_CONFIG = {\n",
    "#     \"vocab_size\": 128_256,           # Vocabulary size\n",
    "#     \"context_length\": 131_072,       # Context length that was used to train the model\n",
    "#     \"emb_dim\": 3072,                 # Embedding dimension\n",
    "#     \"n_heads\": 24,                   # Number of attention heads\n",
    "#     \"n_layers\": 28,                  # Number of layers\n",
    "#     \"hidden_dim\": 8192,              # Size of the intermediate dimension in FeedForward\n",
    "#     \"n_kv_groups\": 8,                # Key-Value groups for grouped-query attention\n",
    "#     \"rope_base\": 500_000.0,          # The base in RoPE's \"theta\"\n",
    "#     \"dtype\": torch.bfloat16,         # Lower-precision dtype to reduce memory usage\n",
    "#     \"rope_freq\": {                   # RoPE frequency scaling\n",
    "#         \"factor\": 32.0,\n",
    "#         \"low_freq_factor\": 1.0,\n",
    "#         \"high_freq_factor\": 4.0,\n",
    "#         \"original_context_length\": 8192,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859b5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama3Model(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d7c979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1498482688\n",
      "\n",
      "Total number of unique parameters: 1235814400\n"
     ]
    }
   ],
   "source": [
    "#Print model parameter size which includes every single parameter\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "#Account for weight tying (i.e. same parameter matrices are used in multiple places in the model architecture)\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "\n",
    "#Unique parameters in the model\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32: 11.23 GB\n",
      "bfloat16: 5.61 GB\n"
     ]
    }
   ],
   "source": [
    "#This function calculates the total memory footprint of the Llama model by accounting for three memory component types: model parameters, parameter gradients & model buffers.\n",
    "\n",
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size=param.numel()\n",
    "        total_params += param_size\n",
    "        #Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    #Size in bytes = (Number of elements) * (Size of each element)\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    #Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32: {model_memory_size(model,input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ae274be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791fd024",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baebb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Thin wrapper around tiktoken that keeps track of Llama-3 special IDs.\"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise FileNotFoundError(model_path)\n",
    "\n",
    "        mergeable = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        # hard-coded from Meta's tokenizer.json\n",
    "        self.special = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        self.special.update({f\"<|reserved_{i}|>\": 128002 + i\n",
    "                             for i in range(256)\n",
    "                             if 128002 + i not in self.special.values()})\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)\"\n",
    "                    r\"|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+\"\n",
    "                    r\"|\\p{N}{1,3}\"\n",
    "                    r\"| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*\"\n",
    "                    r\"|\\s*[\\r\\n]+\"\n",
    "                    r\"|\\s+(?!\\S)\"\n",
    "                    r\"|\\s+\",\n",
    "            mergeable_ranks=mergeable,\n",
    "            special_tokens=self.special,\n",
    "        )\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False):\n",
    "        ids = ([self.special[\"<|begin_of_text|>\"]] if bos else []) \\\n",
    "              + self.model.encode(text)\n",
    "        if eos:\n",
    "            ids.append(self.special[\"<|end_of_text|>\"])\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.model.decode(ids)\n",
    "\n",
    "\n",
    "class ChatFormat:\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer, *,\n",
    "                 default_system=\"You are a helpful assistant.\"):\n",
    "        self.tok = tokenizer\n",
    "        self.default_system = default_system\n",
    "\n",
    "    def _header(self, role):\n",
    "        \"\"\"Encode <|start_header_id|>role<|end_header_id|>\\n\\n\"\"\"\n",
    "        return (\n",
    "            [self.tok.special[\"<|start_header_id|>\"]]\n",
    "            + self.tok.encode(role)\n",
    "            + [self.tok.special[\"<|end_header_id|>\"]]\n",
    "            + self.tok.encode(\"\\n\\n\")\n",
    "        )\n",
    "\n",
    "    def encode(self, user_message, system_message=None):\n",
    "        sys_msg = system_message if system_message is not None else self.default_system\n",
    "\n",
    "        ids = [self.tok.special[\"<|begin_of_text|>\"]]\n",
    "\n",
    "        # system\n",
    "        ids += self._header(\"system\")\n",
    "        ids += self.tok.encode(sys_msg)\n",
    "        ids += [self.tok.special[\"<|eot_id|>\"]]\n",
    "\n",
    "        # user\n",
    "        ids += self._header(\"user\")\n",
    "        ids += self.tok.encode(user_message)\n",
    "        ids += [self.tok.special[\"<|eot_id|>\"]]\n",
    "\n",
    "        # assistant header (no content yet)\n",
    "        ids += self._header(\"assistant\")\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d199d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59d8f1c2a5040f39039470eac4a5bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HuggingFace Account Authentication\n",
    "#Uncomment and run the following code if you are executing the notebook for the first time. It will ask for an access token from huggingface as part of the output.\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8863542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e954060d71e7415db087d98dfec6e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Downloads the official Llama 3.2 tokenizer file from Hugging Face hub to local machine (requires permission)\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "    filename=\"original/tokenizer.model\",\n",
    "    local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e80ae1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Llama 3.2 tokenizer and chat formatting for encoding user/system messages and special tokens.\n",
    "\n",
    "tokenizer = Tokenizer(tokenizer_file_path)\n",
    "chat_tokenizer = ChatFormat(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3945e",
   "metadata": {},
   "source": [
    "## Load pretrained weights\n",
    "\n",
    "Purpose: \n",
    "- This section defines the critical functions needed to bridge Meta's official Llama 3.2 checkpoint format with our custom implementation architecture. The mapping strategy ensures dimensional compatibility and proper parameter assignment between HuggingFace's naming conventions and our step-by-step educational implementation.\n",
    "\n",
    "Core Architecture Mapping Functions: \n",
    "- assign(): Safe tensor assignment with shape validation to ensure dimensional compatibility between model layers and pretrained weights, handling both torch.Tensor and numpy array inputs with detailed error reporting for debugging shape mismatches \n",
    "- load_weights_into_llama(): Complete weight mapping pipeline that systematically maps HuggingFace checkpoint parameter names to this custom architecture, processes all 16 transformer layers (attention + feed-forward + normalization), and handles weight tying scenarios between embedding and output layers\n",
    "\n",
    "Key Architectural Differences Handled: \n",
    "- GQA Compatibility: K/V projections are 4x smaller (8 groups vs 32 heads) requiring careful dimension mapping \n",
    "- SwiGLU Structure: Maps 3 linear layers (gate + up + down) instead of traditional 2-layer feed-forward networks \n",
    "- RMSNorm Integration: Ensures proper normalization layer compatibility across pre-attention and pre-FFN positions \n",
    "- Weight Tying Logic: Supports both tied and untied output head configurations depending on checkpoint format\n",
    "\n",
    "The detailed weight mapping strategy provides the complete reference for understanding how each component of our educational implementation corresponds to Meta's production architecture, enabling seamless transfer of billion-parameter pretrained knowledge into our custom Llama framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff151b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right, tensor_name=\"unknown\"):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # Load attention weights\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # Load FeedForward weights\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # Load output layer weights\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    if \"lm_head.weight\" in params.keys():\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "        print(\"Model uses weight tying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a91d76",
   "metadata": {},
   "source": [
    "## **Pretrained Weight Download and Model Initialization with SafeTensors**\n",
    "\n",
    "**Purpose:** This code downloads Meta's official Llama 3.2 pretrained weights from Hugging Face Hub and loads them into this custom model implementation, transforming it from a randomly initialized neural network into a fully functional language model. The process uses SafeTensors format for secure weight loading and handles both 1B and 3B model variants with different file splitting strategies.\n",
    "\n",
    "**Key Operations and Logic:**\n",
    "• **SafeTensors Import:** Imports the `load_file` function from safetensors library for memory-efficient and secure tensor loading\n",
    "• **Model Size Detection:** Uses conditional logic to check `LLAMA_SIZE_STR` variable - if \"1B\", downloads single weight file; otherwise assumes 3B model requiring multiple files\n",
    "• **1B Model Path:** Downloads `model.safetensors` file (~2.5GB) directly using `hf_hub_download()` from Meta's repository and loads it with `load_file()`\n",
    "• **3B Model Path:** Iterates through files `model-00001-of-00002.safetensors` and `model-00002-of-00002.safetensors`, downloading each separately and combining dictionaries with `update()`\n",
    "• **Weight Integration:** Calls `load_weights_into_llama()` function to map HuggingFace parameter names to our custom architecture layer names\n",
    "• **Device Transfer:** Moves the now-pretrained model to the appropriate computational device (GPU, MPS, or CPU) for inference\n",
    "• **Memory Cleanup:** Deletes the `combined_weights` dictionary to free up system memory after weight transfer is complete\n",
    "\n",
    "The SafeTensors format provides memory mapping, corruption detection, and faster loading compared to traditional pickle files, while the conditional logic elegantly handles different model sizes without code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2941ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac78ab94425e4893b75ecba799eb86f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  35%|###4      | 860M/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uses weight tying.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "if LLAMA_SIZE_STR == \"1B\":\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "        filename=\"model.safetensors\",\n",
    "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "    )\n",
    "    combined_weights = load_file(weights_file)\n",
    "\n",
    "\n",
    "else:\n",
    "    combined_weights = {}\n",
    "    for i in range(1, 3):\n",
    "        weights_file = hf_hub_download(\n",
    "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "            filename=f\"model-0000{i}-of-00002.safetensors\",\n",
    "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "        )\n",
    "        current_weights = load_file(weights_file)\n",
    "        combined_weights.update(current_weights)\n",
    "\n",
    "\n",
    "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
    "model.to(device)\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f518925",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae533121",
   "metadata": {},
   "source": [
    "\n",
    "**Purpose:** This code implements the complete text generation pipeline for the Llama 3.2 model, including utility functions for tokenization conversion and an advanced sampling function that supports multiple generation strategies. The implementation enables the pretrained model to generate coherent text responses with configurable randomness and quality controls through temperature scaling and top-k sampling techniques.\n",
    "\n",
    "**Key Functions and Operations:**\n",
    "• **text_to_token_ids():** Converts input text strings into tensor format by encoding with the tokenizer and adding batch dimension for model processing\n",
    "• **token_ids_to_text():** Converts model output token tensors back to readable text by removing batch dimension and decoding through tokenizer\n",
    "• **generate():** Core autoregressive generation function that iteratively produces new tokens using the model's forward pass with gradient computation disabled for efficiency\n",
    "• **Context Window Management:** Truncates input sequence to fit within model's context size limit using `idx[:, -context_size:]` slicing\n",
    "• **Top-k Sampling:** Filters vocabulary to keep only the k most probable tokens, setting others to negative infinity to prevent selection\n",
    "• **Temperature Scaling:** Controls generation randomness by dividing logits by temperature value - lower values increase determinism, higher values increase creativity\n",
    "• **Sampling Strategies:** Supports both deterministic generation (argmax) when temperature=0 and probabilistic sampling (multinomial) when temperature>0\n",
    "• **Early Stopping:** Implements optional end-of-sequence token detection to halt generation when model signals completion\n",
    "\n",
    "**Technical Implementation:** The function operates through an autoregressive loop where each iteration generates one new token based on the current sequence, appends it to the growing output, and continues until reaching the maximum token limit or encountering a stop condition. The torch.no_grad() context ensures memory efficiency during inference by preventing gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad587351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a8cb5",
   "metadata": {},
   "source": [
    "## **Llama 3.2 Text Generation Demo with Performance Monitoring**\n",
    "\n",
    "**Purpose:** This code demonstrates the complete text generation workflow using the pretrained Llama 3.2 model with a sample prompt, while measuring inference performance and memory usage. It showcases the model's ability to generate coherent responses using deterministic sampling and includes post-processing to extract clean output from the chat format.\n",
    "\n",
    "**Key Operations and Workflow:**\n",
    "• **Prompt Setup:** Defines a simple question \"What do llamas eat?\" as the input prompt for demonstration\n",
    "• **Reproducibility:** Sets torch random seed to 123 for consistent generation results across runs\n",
    "• **Text Generation:** Calls the generate() function with deterministic settings (top_k=1, temperature=0.0) to produce the most probable token sequence for 150 new tokens maximum\n",
    "• **Input Processing:** Converts the prompt text to token IDs using chat_tokenizer and transfers to appropriate device (GPU/MPS/CPU)\n",
    "• **Performance Measurement:** Records generation time using time.time() to benchmark inference speed\n",
    "• **Memory Monitoring:** Tracks maximum GPU memory allocation if CUDA is available for resource usage analysis\n",
    "• **Output Conversion:** Transforms generated token IDs back to readable text using the tokenizer\n",
    "• **Text Cleaning:** Applies clean_text() function to extract the assistant's response by removing chat format headers and special tokens\n",
    "\n",
    "**Technical Configuration:** The generation uses greedy decoding (temperature=0.0) with top-k=1 sampling for deterministic output, processes up to the full context length of 131,072 tokens, and leverages the chat tokenizer format to properly structure the conversation with system/user/assistant roles. The performance metrics help evaluate the model's efficiency on the current hardware setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17f8b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6.47 sec\n",
      "\n",
      "\n",
      "Output text:\n",
      "\n",
      " The core symptoms of depression can vary from person to person, but here are some common ones:\n",
      "\n",
      "**Primary Symptoms:**\n",
      "\n",
      "1. **Feeling sad, empty, or hopeless**: A persistent feeling of sadness, emptiness, or hopelessness that interferes with daily life.\n",
      "2. **Loss of interest in activities**: A lack of interest or pleasure in activities that once brought joy, such as hobbies, socializing, or sex.\n",
      "3. **Changes in appetite or sleep**: Significant changes in appetite or sleep patterns, such as overeating or insomnia.\n",
      "4. **Fatigue or low energy**: Feeling tired, sluggish, or lacking the energy to perform daily tasks.\n",
      "5. **Difficulty concentrating or making decisions**: Trouble focusing, making decisions, or\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "PROMPT = \"What are the core symptoms of depression?\"\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(PROMPT, chat_tokenizer).to(device),\n",
    "    max_new_tokens=150,\n",
    "    context_size=LLAMA32_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(f\"Time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    max_mem_bytes = torch.cuda.max_memory_allocated()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")\n",
    "\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "\n",
    "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
    "    # Find the index of the first occurrence of \"<|end_header_id|>\"\n",
    "    index = text.find(header_end)\n",
    "\n",
    "    if index != -1:\n",
    "        # Return the substring starting after \"<|end_header_id|>\"\n",
    "        return text[index + len(header_end):].strip()  # Strip removes leading/trailing whitespace\n",
    "    else:\n",
    "        # If the token is not found, return the original text\n",
    "        return text\n",
    "\n",
    "print(\"\\n\\nOutput text:\\n\\n\", clean_text(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adb808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
