{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629c0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "numpy version: 2.2.6\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.8.0\n",
      "tensorflow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\" #For OpenAI's pretrained weights\n",
    "       ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414cd78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anup/gitProjects/language-models-psychiatry\n"
     ]
    }
   ],
   "source": [
    "#working directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bc60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gpt_from_scratch.GPTModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gpt_from_scratch import GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f22461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class GPTModel(nn.Module):\n",
       "    def __init__(self, cfg):\n",
       "        super().__init__()\n",
       "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
       "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
       "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
       "\n",
       "        self.trf_blocks = nn.Sequential(\n",
       "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
       "\n",
       "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
       "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
       "\n",
       "    def forward(self, in_idx):\n",
       "        batch_size, seq_len = in_idx.shape\n",
       "        tok_embeds = self.tok_emb(in_idx)\n",
       "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
       "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
       "        x = self.drop_emb(x)\n",
       "        x = self.trf_blocks(x)\n",
       "        x = self.final_norm(x)\n",
       "        logits = self.out_head(x)\n",
       "        return logits\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Inspect the GPT Model\n",
    "import inspect\n",
    "from gpt_from_scratch import GPTModel\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(GPTModel)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e26414f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config file that contains model parameters for GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d99a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  #Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcceefa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#View of the model architecture\n",
    "from IPython.display import display\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f792c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([256, 768])\n",
      "trf_blocks.0.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.0.norm1.scale torch.Size([768])\n",
      "trf_blocks.0.norm1.shift torch.Size([768])\n",
      "trf_blocks.0.norm2.scale torch.Size([768])\n",
      "trf_blocks.0.norm2.shift torch.Size([768])\n",
      "trf_blocks.1.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.1.norm1.scale torch.Size([768])\n",
      "trf_blocks.1.norm1.shift torch.Size([768])\n",
      "trf_blocks.1.norm2.scale torch.Size([768])\n",
      "trf_blocks.1.norm2.shift torch.Size([768])\n",
      "trf_blocks.2.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.2.norm1.scale torch.Size([768])\n",
      "trf_blocks.2.norm1.shift torch.Size([768])\n",
      "trf_blocks.2.norm2.scale torch.Size([768])\n",
      "trf_blocks.2.norm2.shift torch.Size([768])\n",
      "trf_blocks.3.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.3.norm1.scale torch.Size([768])\n",
      "trf_blocks.3.norm1.shift torch.Size([768])\n",
      "trf_blocks.3.norm2.scale torch.Size([768])\n",
      "trf_blocks.3.norm2.shift torch.Size([768])\n",
      "trf_blocks.4.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.4.norm1.scale torch.Size([768])\n",
      "trf_blocks.4.norm1.shift torch.Size([768])\n",
      "trf_blocks.4.norm2.scale torch.Size([768])\n",
      "trf_blocks.4.norm2.shift torch.Size([768])\n",
      "trf_blocks.5.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.5.norm1.scale torch.Size([768])\n",
      "trf_blocks.5.norm1.shift torch.Size([768])\n",
      "trf_blocks.5.norm2.scale torch.Size([768])\n",
      "trf_blocks.5.norm2.shift torch.Size([768])\n",
      "trf_blocks.6.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.6.norm1.scale torch.Size([768])\n",
      "trf_blocks.6.norm1.shift torch.Size([768])\n",
      "trf_blocks.6.norm2.scale torch.Size([768])\n",
      "trf_blocks.6.norm2.shift torch.Size([768])\n",
      "trf_blocks.7.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.7.norm1.scale torch.Size([768])\n",
      "trf_blocks.7.norm1.shift torch.Size([768])\n",
      "trf_blocks.7.norm2.scale torch.Size([768])\n",
      "trf_blocks.7.norm2.shift torch.Size([768])\n",
      "trf_blocks.8.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.8.norm1.scale torch.Size([768])\n",
      "trf_blocks.8.norm1.shift torch.Size([768])\n",
      "trf_blocks.8.norm2.scale torch.Size([768])\n",
      "trf_blocks.8.norm2.shift torch.Size([768])\n",
      "trf_blocks.9.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.9.norm1.scale torch.Size([768])\n",
      "trf_blocks.9.norm1.shift torch.Size([768])\n",
      "trf_blocks.9.norm2.scale torch.Size([768])\n",
      "trf_blocks.9.norm2.shift torch.Size([768])\n",
      "trf_blocks.10.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.10.norm1.scale torch.Size([768])\n",
      "trf_blocks.10.norm1.shift torch.Size([768])\n",
      "trf_blocks.10.norm2.scale torch.Size([768])\n",
      "trf_blocks.10.norm2.shift torch.Size([768])\n",
      "trf_blocks.11.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.11.norm1.scale torch.Size([768])\n",
      "trf_blocks.11.norm1.shift torch.Size([768])\n",
      "trf_blocks.11.norm2.scale torch.Size([768])\n",
      "trf_blocks.11.norm2.shift torch.Size([768])\n",
      "final_norm.scale torch.Size([768])\n",
      "final_norm.shift torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n",
      "\n",
      "number of parameter layers: 161\n"
     ]
    }
   ],
   "source": [
    "#View of the model's parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "print(f\"\\nnumber of parameter layers: {len(list(model.named_parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f395407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
       "    # idx is (B, T) array of indices in the current context\n",
       "    for _ in range(max_new_tokens):\n",
       "\n",
       "        # Crop current context if it exceeds the supported context size\n",
       "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
       "        # then only the last 5 tokens are used as context\n",
       "        idx_cond = idx[:, -context_size:]\n",
       "\n",
       "        # Get the predictions\n",
       "        with torch.no_grad():\n",
       "            logits = model(idx_cond)\n",
       "\n",
       "        # Focus only on the last time step\n",
       "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
       "        logits = logits[:, -1, :]\n",
       "\n",
       "        # Get the idx of the vocab entry with the highest logits value\n",
       "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
       "\n",
       "        # Append sampled index to the running sequence\n",
       "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
       "\n",
       "    return idx\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This block of code follows after viewing the GPT-2 model architecture and weights to transition from model inspection to practical usage. It ensures the tokenizer is available, adds the current directory to the Python path so local modules can be imported, and then imports the generate_text_simple function, which is essential for generating new text with the model.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Add the current notebook's directory to sys.path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "#Import generate_text_simple function from gpt_from_scratch.py\n",
    "from gpt_from_scratch import generate_text_simple\n",
    "\n",
    "#Visualize the function\n",
    "import inspect\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(generate_text_simple)}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0d0064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#This block demonstrates how to use the loaded GPT-2 model for text generation. It defines helper functions to convert text to token IDs and back (i.e. encode and decode) and then uses the generate_text_simple function to generate next tokens from the model. Finally, it decodes and prints the generated output as readable text, showing the practical application of th emodel for generating language.\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\",token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines two small batch of tokenized inputs and their corresponding target sequences. The targets are the next tokens for each input, which is is standard in language modeling.\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#Decodes a sequence of token IDS from a previous generation step into human readable text using the tokenizer.\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307844e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#Passes the inputs batch through the GPT model to get raw output scores (logits) for each token position and vocabulary entry. Applies softmax to convert logist to probabilities over the vocabulary for each token position.\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c321b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#For each token position, selects the token ID with the highest probability. token_ids now contains the model's predicted next token for each position in the batch.\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae211eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "#Decodes the first row of targets and model's predicted token_ids back to text for easy comparison. Allows one to see how well the model's predictions (in this untrained model) match the expected next tokens.\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a5ec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "#Extracts probabilities assigned by the model to the correct or next tokens for each position in both input sequences\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Computes the log-probabilities of the correct tokens and then averages them. The average log-probability is a standard metric for model performance (higher is better) and is the basis for cross entropy calculations.\n",
    "\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b55869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Negates the average log-probability to convert it into a loss (since optimization frameworks minimize loss. This is the cross-entropy value, the standard function for language modeling.\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape of the inputs and outputs.\n",
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a989ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Flatten tensors by combining them over the batch dimension.\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0fa7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Using PyTorch built-in cross-entropy function to compute the loss in a numerically stable and efficient way.\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "#Calculates perplexity, a commonly used metric in language modeling that is the exponentiated cross-entrophy loss. (i.e. lower perplexity means the model is more confident and accurate in its predictions)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac100f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and load a small text dataset for training and validation\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "148e4333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "#Checking the text\n",
    "print(text_data[:99])\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "#Text length in terms of characters and tokens\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2970816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
