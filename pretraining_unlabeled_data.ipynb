{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7db9585",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#228B22; font-weight:bold; margin-bottom:0.5em;\">Pretraining a GPT-Style Language Model from Scratch</h1>\n",
    "\n",
    "<p>\n",
    "<strong>Author:</strong> <span style=\"color:#FFD800;\">Anup Sharma</span><br>\n",
    "</p>\n",
    "\n",
    "<hr style=\"border-top: 2px solid #888888;\">\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><strong>Dataset</strong></td>\n",
    "    <td>Characters: <span style=\"color:#228B22;\">10,419</span></td>\n",
    "    <td>Tokens: <span style=\"color:#228B22;\">2,772</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Model</strong></td>\n",
    "    <td>Type: <span style=\"color:#228B22;\">GPT (Transformer)</span></td>\n",
    "    <td>Layers: <span style=\"color:#228B22;\">12</span></td>\n",
    "    <td>Heads: <span style=\"color:#228B22;\">12</span></td>\n",
    "    <td>Embedding Dim: <span style=\"color:#228B22;\">768</span></td>\n",
    "    <td>Context Length: <span style=\"color:#228B22;\">256</span></td>\n",
    "    <td>Epochs: <span style=\"color:#228B22;\">10</span></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<hr style=\"border-top: 2px solid #888888;\">\n",
    "\n",
    "<p style=\"font-size:1.1em; margin-bottom:0.5em;\"><strong>Workflow Overview:</strong></p>\n",
    "<p>\n",
    "  This notebook demonstrates how to pretrain a GPT-style language model from scratch on a small sample text dataset. The data is split into training and validation sets, and the model is trained for 10 epochs using the AdamW optimizer. At each epoch, the model processes data in batches, computes cross-entropy loss, updates its weights, periodically evaluates performance on both training and validation sets, and generates sample text to monitor progress. This workflow leverages all core features of the transformer architecture and provides a hands-on introduction to large language model pretraining.\n",
    "</p>\n",
    "\n",
    "<p style=\"color:#888;\">\n",
    "  <em>Run the cells below to inspect the dataset, model, and training process from scratch for educational purposes and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629c0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.8.0\n",
      "tensorflow version: 2.16.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\" #For OpenAI's pretrained weights\n",
    "       ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23c9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path added to sys.path: /Users/anup/gitProjects/language-models-psychiatry\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "repo_path = \"/Users/anup/gitProjects/language-models-psychiatry\"\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "print(\"Repo path added to sys.path:\", repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795cc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/anup/gitProjects/language-models-psychiatry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9edd5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_from_scratch.py exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"gpt_from_scratch.py exists:\", os.path.exists(\"gpt_from_scratch.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "34f22461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class GPTModel(nn.Module):\n",
       "    def __init__(self, cfg):\n",
       "        super().__init__()\n",
       "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
       "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
       "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
       "\n",
       "        self.trf_blocks = nn.Sequential(\n",
       "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
       "\n",
       "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
       "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
       "\n",
       "    def forward(self, in_idx):\n",
       "        batch_size, seq_len = in_idx.shape\n",
       "        tok_embeds = self.tok_emb(in_idx)\n",
       "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
       "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
       "        x = self.drop_emb(x)\n",
       "        x = self.trf_blocks(x)\n",
       "        x = self.final_norm(x)\n",
       "        logits = self.out_head(x)\n",
       "        return logits\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import torch and nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Inspect the GPT Model\n",
    "import inspect\n",
    "from gpt_from_scratch import GPTModel\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(GPTModel)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26414f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config file that contains model parameters for GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d99a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  #Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcceefa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#View of the model architecture\n",
    "from IPython.display import display\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f792c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([256, 768])\n",
      "trf_blocks.0.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.0.norm1.scale torch.Size([768])\n",
      "trf_blocks.0.norm1.shift torch.Size([768])\n",
      "trf_blocks.0.norm2.scale torch.Size([768])\n",
      "trf_blocks.0.norm2.shift torch.Size([768])\n",
      "trf_blocks.1.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.1.norm1.scale torch.Size([768])\n",
      "trf_blocks.1.norm1.shift torch.Size([768])\n",
      "trf_blocks.1.norm2.scale torch.Size([768])\n",
      "trf_blocks.1.norm2.shift torch.Size([768])\n",
      "trf_blocks.2.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.2.norm1.scale torch.Size([768])\n",
      "trf_blocks.2.norm1.shift torch.Size([768])\n",
      "trf_blocks.2.norm2.scale torch.Size([768])\n",
      "trf_blocks.2.norm2.shift torch.Size([768])\n",
      "trf_blocks.3.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.3.norm1.scale torch.Size([768])\n",
      "trf_blocks.3.norm1.shift torch.Size([768])\n",
      "trf_blocks.3.norm2.scale torch.Size([768])\n",
      "trf_blocks.3.norm2.shift torch.Size([768])\n",
      "trf_blocks.4.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.4.norm1.scale torch.Size([768])\n",
      "trf_blocks.4.norm1.shift torch.Size([768])\n",
      "trf_blocks.4.norm2.scale torch.Size([768])\n",
      "trf_blocks.4.norm2.shift torch.Size([768])\n",
      "trf_blocks.5.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.5.norm1.scale torch.Size([768])\n",
      "trf_blocks.5.norm1.shift torch.Size([768])\n",
      "trf_blocks.5.norm2.scale torch.Size([768])\n",
      "trf_blocks.5.norm2.shift torch.Size([768])\n",
      "trf_blocks.6.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.6.norm1.scale torch.Size([768])\n",
      "trf_blocks.6.norm1.shift torch.Size([768])\n",
      "trf_blocks.6.norm2.scale torch.Size([768])\n",
      "trf_blocks.6.norm2.shift torch.Size([768])\n",
      "trf_blocks.7.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.7.norm1.scale torch.Size([768])\n",
      "trf_blocks.7.norm1.shift torch.Size([768])\n",
      "trf_blocks.7.norm2.scale torch.Size([768])\n",
      "trf_blocks.7.norm2.shift torch.Size([768])\n",
      "trf_blocks.8.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.8.norm1.scale torch.Size([768])\n",
      "trf_blocks.8.norm1.shift torch.Size([768])\n",
      "trf_blocks.8.norm2.scale torch.Size([768])\n",
      "trf_blocks.8.norm2.shift torch.Size([768])\n",
      "trf_blocks.9.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.9.norm1.scale torch.Size([768])\n",
      "trf_blocks.9.norm1.shift torch.Size([768])\n",
      "trf_blocks.9.norm2.scale torch.Size([768])\n",
      "trf_blocks.9.norm2.shift torch.Size([768])\n",
      "trf_blocks.10.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.10.norm1.scale torch.Size([768])\n",
      "trf_blocks.10.norm1.shift torch.Size([768])\n",
      "trf_blocks.10.norm2.scale torch.Size([768])\n",
      "trf_blocks.10.norm2.shift torch.Size([768])\n",
      "trf_blocks.11.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.11.norm1.scale torch.Size([768])\n",
      "trf_blocks.11.norm1.shift torch.Size([768])\n",
      "trf_blocks.11.norm2.scale torch.Size([768])\n",
      "trf_blocks.11.norm2.shift torch.Size([768])\n",
      "final_norm.scale torch.Size([768])\n",
      "final_norm.shift torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n",
      "\n",
      "number of parameter layers: 161\n"
     ]
    }
   ],
   "source": [
    "#View of the model's parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "print(f\"\\nnumber of parameter layers: {len(list(model.named_parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f395407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
       "    # idx is (B, T) array of indices in the current context\n",
       "    for _ in range(max_new_tokens):\n",
       "\n",
       "        # Crop current context if it exceeds the supported context size\n",
       "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
       "        # then only the last 5 tokens are used as context\n",
       "        idx_cond = idx[:, -context_size:]\n",
       "\n",
       "        # Get the predictions\n",
       "        with torch.no_grad():\n",
       "            logits = model(idx_cond)\n",
       "\n",
       "        # Focus only on the last time step\n",
       "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
       "        logits = logits[:, -1, :]\n",
       "\n",
       "        # Get the idx of the vocab entry with the highest logits value\n",
       "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
       "\n",
       "        # Append sampled index to the running sequence\n",
       "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
       "\n",
       "    return idx\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This block of code follows after viewing the GPT-2 model architecture and weights to transition from model inspection to practical usage. It ensures the tokenizer is available, adds the current directory to the Python path so local modules can be imported, and then imports the generate_text_simple function, which is essential for generating new text with the model.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Add the current notebook's directory to sys.path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "#Import generate_text_simple function from gpt_from_scratch.py\n",
    "from gpt_from_scratch import generate_text_simple\n",
    "\n",
    "#Visualize the function\n",
    "import inspect\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(generate_text_simple)}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0d0064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#This block demonstrates how to use the loaded GPT-2 model for text generation. It defines helper functions to convert text to token IDs and back (i.e. encode and decode) and then uses the generate_text_simple function to generate next tokens from the model. Finally, it decodes and prints the generated output as readable text, showing the practical application of th emodel for generating language.\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\",token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines two small batch of tokenized inputs and their corresponding target sequences. The targets are the next tokens for each input, which is is standard in language modeling.\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03a2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#Decodes a sequence of token IDS from a previous generation step into human readable text using the tokenizer.\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307844e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#Passes the inputs batch through the GPT model to get raw output scores (logits) for each token position and vocabulary entry. Applies softmax to convert logist to probabilities over the vocabulary for each token position.\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c321b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#For each token position, selects the token ID with the highest probability. token_ids now contains the model's predicted next token for each position in the batch.\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae211eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "#Decodes the first row of targets and model's predicted token_ids back to text for easy comparison. Allows one to see how well the model's predictions (in this untrained model) match the expected next tokens.\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a5ec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "#Extracts probabilities assigned by the model to the correct or next tokens for each position in both input sequences\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e04307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Computes the log-probabilities of the correct tokens and then averages them. The average log-probability is a standard metric for model performance (higher is better) and is the basis for cross entropy calculations.\n",
    "\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b55869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Negates the average log-probability to convert it into a loss (since optimization frameworks minimize loss. This is the cross-entropy value, the standard function for language modeling.\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "907a750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape of the inputs and outputs.\n",
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a989ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Flatten tensors by combining them over the batch dimension.\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fa7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Using PyTorch built-in cross-entropy function to compute the loss in a numerically stable and efficient way.\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca12cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "#Calculates perplexity, a commonly used metric in language modeling that is the exponentiated cross-entrophy loss. (i.e. lower perplexity means the model is more confident and accurate in its predictions)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac100f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and load a small text dataset for training and validation\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "148e4333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "#Checking the text\n",
    "print(text_data[:99])\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1186454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "#Text length in terms of characters and tokens\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8134091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into a training and validation set and use the data loaders to prepare the batches for language model training\n",
    "\n",
    "from gpt_from_scratch import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2970816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for errors\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c63fcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"Validation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fee5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens are: 4608 + 512 or 5120\n"
     ]
    }
   ],
   "source": [
    "#Confirming token sizes are in the expected range.\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(f\"All tokens are: {train_tokens} + {val_tokens} or\",  train_tokens + val_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4c188ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define utility functions to compute loss for a batch or an entire DataLoader\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0074b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "Training loss: 10.987582948472765\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "#Moves the model to the appropriate device (CPU/GPU/MPS), computes and prints the intitial training and validation losses. This establishes a baseline before training.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): #Disable gradient tracking not yet training\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "657c1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients from prior batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculates the gradients\n",
    "            optimizer.step()  # Update model weights\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "        # Optional evaluation step\n",
    "        if global_step % eval_freq == 0:\n",
    "            train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            track_tokens_seen.append(tokens_seen)\n",
    "            print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                  f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceacfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000008): Train loss 7.310, Val loss 7.465\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Every effort moves you.                                                 \n",
      "Every effort moves you.                           \" I had a I had\" I had the picture had to the picture and I had to me--I\n",
      "Ep 5 (Step 000044): Train loss 3.551, Val loss 6.204\n",
      "Every effort moves you know to                                                \n",
      "Every effort moves you know it to see a little to have to the picture--I looked of that he had been--I had to see it was to see the donkey, and in the picture at my elbow and in the donkey, and down the room, with the\n",
      "Every effort moves you know it was not that the picture for a smile that, I was not--so it was no--the the fact, with a little a smile behind his pictures--I had been his own he had the donkey.      \n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.                  He placed them at my elbow and as his pictures? a and were, and in his\n",
      "Ep 9 (Step 000080): Train loss 0.912, Val loss 6.277\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.     \"I didn't you know, and threw back the head to look up at the honour being _mine_--because he's the first\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.28 minutes.\n"
     ]
    }
   ],
   "source": [
    "#Now train model with the execution time\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = training_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=4, eval_iter=4,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time)/ 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f604475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVJlJREFUeJzt3XlcVGX7x/HPsC+y76ggAoKighvmvkCuYWqWlZW2PiluWWZlufUrM80sK9ue9KlMW1XcMvd9wQXUVNxwBcQNARUE5v79MTAy7iI4M3i9X69TzDlnztz3zMiXs1zn1iilFEIIIYQwSRbGboAQQgghbk6CWgghhDBhEtRCCCGECZOgFkIIIUyYBLUQQghhwiSohRBCCBMmQS2EEEKYMAlqIYQQwoRJUAshhBAmTIJaCDNz5MgRNBoNSUlJxm6KEOI+kKAWwgg0Gs0tpzFjxhi7iUIIE2Fl7AYI8SBKT0/X//zrr78yatQoUlJS9POqVKlijGYJIUyQ7FELYQS+vr76ycXFBY1Go3/s7e3N5MmTqVatGra2tkRFRfH333/fdFtFRUW88MILhIeHc+zYMQDmzZtHw4YNsbOzo2bNmowdO5bCwkL9czQaDd9//z09evTAwcGB0NBQEhIS9MvPnz9Pnz598PLywt7entDQUKZPn37TNvzxxx/Uq1cPe3t7PDw8iI2N5eLFi/rl33//PbVr18bOzo7w8HC++uorg+cfP36cJ554AldXV9zd3Xn00Uc5cuSIfnm/fv3o3r07kyZNws/PDw8PD+Lj4ykoKLjj91wIs6WEEEY1ffp05eLion88efJk5ezsrGbNmqX27dun3nzzTWVtba3279+vlFIqNTVVAWrHjh0qLy9P9ejRQzVo0EBlZmYqpZRas2aNcnZ2VjNmzFCHDh1S//zzj6pRo4YaM2aM/jUAVa1aNfXLL7+oAwcOqMGDB6sqVaqos2fPKqWUio+PV1FRUSoxMVGlpqaqpUuXqoSEhBu2Py0tTVlZWanJkyer1NRUtXPnTvXll1+qnJwcpZRSP//8s/Lz81N//vmnOnz4sPrzzz+Vu7u7mjFjhlJKqStXrqjatWurF154Qe3cuVPt2bNHPf300yosLEzl5+crpZTq27evcnZ2Vq+++qrau3evmj9/vnJwcFDffvtt+X4YQpggCWohjOzaoPb391cffPCBwTpNmjRRAwYMUEpdDeq1a9eqmJgY1bJlS5WVlaVfNyYmRn344YcGz//pp5+Un5+f/jGg3n33Xf3j3NxcBajFixcrpZSKi4tTzz///B21f9u2bQpQR44cueHy4OBg9csvvxjMe//991WzZs30bQsLC1NarVa/PD8/X9nb26slS5YopXRBHRgYqAoLC/XrPP7446p379531EYhzJmcoxbChGRnZ5OWlkaLFi0M5rdo0YLk5GSDeU899RTVqlVjxYoV2Nvb6+cnJyezfv16PvjgA/28oqIi8vLyuHTpEg4ODgDUr19fv9zR0RFnZ2cyMzMB6N+/P4899hjbt2+nQ4cOdO/enebNm9+wzZGRkcTExFCvXj06duxIhw4d6NWrF25ubly8eJFDhw7x4osv8vLLL+ufU1hYiIuLi769Bw8exMnJyWC7eXl5HDp0SP84IiICS0tL/WM/Pz927dp1i3dTiMpBgloIM9WlSxd+/vlnNm7cSPv27fXzc3NzGTt2LD179rzuOXZ2dvqfra2tDZZpNBq0Wi0AnTt35ujRoyxatIilS5cSExNDfHw8kyZNum6blpaWLF26lA0bNvDPP/8wdepURo4cyebNm/V/FHz33Xc0bdr0uueVtLdRo0bMnDnzum17eXndUXuFqMwkqIUwIc7Ozvj7+7N+/XratGmjn79+/Xqio6MN1u3fvz9169alW7duLFy4UL9+w4YNSUlJISQk5J7a4uXlRd++fenbty+tWrVi+PDhNwxq0IVmixYtaNGiBaNGjSIwMJA5c+YwbNgw/P39OXz4MH369Lnhcxs2bMivv/6Kt7c3zs7O99RmISojCWohTMzw4cMZPXo0wcHBREVFMX36dJKSkm64xzlo0CCKiop45JFHWLx4MS1btmTUqFE88sgjBAQE0KtXLywsLEhOTmb37t383//93x21YdSoUTRq1IiIiAjy8/NZsGABtWvXvuG6mzdvZvny5XTo0AFvb282b97M6dOn9euPHTuWwYMH4+LiQqdOncjPz2fr1q2cP3+eYcOG0adPHyZOnMijjz7KuHHjqFatGkePHuWvv/7izTffpFq1amV/M4WoBCSohTAxgwcP5sKFC7z++utkZmZSp04dEhISCA0NveH6Q4cORavV0qVLF/7++286duzIggULGDduHBMmTMDa2prw8HBeeumlO26DjY0Nb7/9NkeOHMHe3p5WrVoxe/bsG67r7OzMmjVrmDJlCtnZ2QQGBvLJJ5/QuXNnAF566SUcHByYOHEiw4cPx9HRkXr16jF06FAAHBwcWLNmDSNGjKBnz57k5ORQtWpVYmJiZA9bCECjlFLGboQQQgghbkxueCKEEEKYMAlqIYQQwoRJUAshhBAmTIJaCCGEMGES1EIIIYQJk6AWQgghTFilD+oxY8ag0WgMpvDwcP3yvLw84uPj8fDwoEqVKjz22GOcOnXKYBvHjh2ja9euODg44O3tzfDhww2GDARYtWoVDRs2xNbWlpCQEGbMmFFufVizZg1xcXH4+/uj0WiYO3euwXKlFKNGjcLPzw97e3tiY2M5cOCAwTrnzp2jT58+ODs74+rqyosvvkhubq7BOjt37qRVq1bY2dlRvXp1Pv744+va8vvvvxMeHo6dnR316tVj0aJFFdKnfv36Xfe5derUyaT7NH78eJo0aYKTkxPe3t50797dYIxpuL/fty+//JIaNWpgZ2dH06ZN2bJlS4X0qW3bttd9Vq+++qrJ9mnatGnUr18fZ2dnnJ2dadasGYsXL9YvN7fP6E77ZW6f04189NFHaDQafQ0+mO/ndVeMPChIhRs9erSKiIhQ6enp+un06dP65a+++qqqXr26Wr58udq6dat66KGHVPPmzfXLCwsLVd26dVVsbKzasWOHWrRokfL09FRvv/22fp3Dhw8rBwcHNWzYMLVnzx41depUZWlpqf7+++9y6cOiRYvUyJEj1V9//aUANWfOHIPlH330kXJxcVFz585VycnJqlu3biooKEhdvnxZv06nTp1UZGSk2rRpk1q7dq0KCQlRTz31lH75hQsXlI+Pj+rTp4/avXu3mjVrlrK3t1fffPONfp3169crS0tL9fHHH6s9e/aod999V1lbW6tdu3aVe5/69u2rOnXqZPC5nTt3zmAdU+tTx44d1fTp09Xu3btVUlKS6tKliwoICFC5ubn6de7X92327NnKxsZG/fDDD+rff/9VL7/8snJ1dVWnTp0q9z61adNGvfzyywaf1YULF0y2TwkJCWrhwoVq//79KiUlRb3zzjvK2tpa7d69Wyllfp/RnfbL3D6na23ZskXVqFFD1a9fXw0ZMkQ/31w/r7vxQAR1ZGTkDZdlZWUpa2tr9fvvv+vn7d27VwFq48aNSildoFhYWKiMjAz9OtOmTVPOzs76sXLffPNNFRERYbDt3r17q44dO5Zzb9R1oabVapWvr6+aOHGiQb9sbW3VrFmzlFJK7dmzRwEqMTFRv87ixYuVRqNRJ0+eVEop9dVXXyk3Nzd9n5RSasSIESosLEz/+IknnlBdu3Y1aE/Tpk3Vf/7zn3Ltk1K6oH700Udv+hxT75NSSmVmZipArV69Wil1f79v0dHRKj4+Xv+4qKhI+fv7q/Hjx5drn5TSBUDpX5zXMvU+KaWUm5ub+v777yvFZ3Sjfill3p9TTk6OCg0NVUuXLjXoR2X7vG6m0h/6Bjhw4AD+/v7UrFmTPn36cOzYMQC2bdtGQUEBsbGx+nXDw8MJCAhg48aNAGzcuJF69erh4+OjX6djx45kZ2fz77//6tcpvY2SdUq2UZFSU1PJyMgweH0XFxeaNm1q0AdXV1caN26sXyc2NhYLCws2b96sX6d169bY2NgY9CElJYXz58/r17mf/Vy1ahXe3t6EhYXRv39/zp49q19mDn26cOECAO7u7sD9+75duXKFbdu2GaxjYWFBbGzsPffr2j6VmDlzJp6entStW5e3336bS5cu6ZeZcp+KioqYPXs2Fy9epFmzZpXiM7pRv0qY6+cUHx9P165dr3vtyvJ53U6lv9d306ZNmTFjBmFhYaSnpzN27FhatWrF7t27ycjIwMbGBldXV4Pn+Pj4kJGRAUBGRobBB1yyvGTZrdbJzs7m8uXLBmMFl7eSNtzo9Uu3z9vb22C5lZUV7u7uBusEBQVdt42SZW5ubjftZ8k2ylOnTp3o2bMnQUFBHDp0iHfeeYfOnTuzceNGLC0tTb5PWq2WoUOH0qJFC+rWrat/zfvxfTt//jxFRUU3XGffvn3l2ieAp59+msDAQPz9/dm5cycjRowgJSWFv/76y2T7tGvXLpo1a0ZeXh5VqlRhzpw51KlTh6SkJLP+jG7WLzDPzwlg9uzZbN++ncTExOuWmfu/qTtV6YO6ZGAAgPr169O0aVMCAwP57bffKjRAxb158skn9T/Xq1eP+vXrExwczKpVq4iJiTFiy+5MfHw8u3fvZt26dcZuSrm5WZ9eeeUV/c/16tXDz8+PmJgYDh06RHBw8P1u5h0JCwsjKSmJCxcu8Mcff9C3b19Wr15t7Gbds5v1q06dOmb5OR0/fpwhQ4awdOlSg7HUHzQPxKHv0lxdXalVqxYHDx7E19eXK1eukJWVZbDOqVOn8PX1BcDX1/e6KwhLHt9uHWdn5wr/Y6CkDTd6/dLty8zMNFheWFjIuXPnyqWfJcsrUs2aNfH09OTgwYP6tphqnwYOHMiCBQtYuXKlwRCN9+v75unpiaWlZbn262Z9upGmTZsCGHxWptYnGxsbQkJCaNSoEePHjycyMpLPPvvMrD+jW/XrRszhc9q2bRuZmZk0bNgQKysrrKysWL16NZ9//jlWVlb4+PiY9ed1px64oM7NzeXQoUP4+fnRqFEjrK2tWb58uX55SkoKx44d05/XadasGbt27TIIhaVLl+Ls7Kw/pNSsWTODbZSsU/rcUEUJCgrC19fX4PWzs7PZvHmzQR+ysrLYtm2bfp0VK1ag1Wr1/1ibNWvGmjVrKCgoMOhDWFgYbm5u+nWM1c8TJ05w9uxZ/Pz89G0xtT4ppRg4cCBz5sxhxYoV1x12v1/fNxsbGxo1amSwjlarZfny5Xfdr9v16UaSkpIADD4rU+rTjWi1WvLz883yM7qTft2IOXxOMTEx7Nq1i6SkJP3UuHFj+vTpo/+5Mn1eN1Xhl6sZ2euvv65WrVqlUlNT1fr161VsbKzy9PRUmZmZSindpf0BAQFqxYoVauvWrapZs2aqWbNm+ueXXNrfoUMHlZSUpP7++2/l5eV1w0v7hw8frvbu3au+/PLLci3PysnJUTt27FA7duxQgJo8ebLasWOHOnr0qFJKV57l6uqq5s2bp3bu3KkeffTRG5ZnNWjQQG3evFmtW7dOhYaGGpQyZWVlKR8fH/Xss8+q3bt3q9mzZysHB4frSpmsrKzUpEmT1N69e9Xo0aPLXMp0qz7l5OSoN954Q23cuFGlpqaqZcuWqYYNG6rQ0FCVl5dnsn3q37+/cnFxUatWrTIogbl06ZJ+nfv1fZs9e7aytbVVM2bMUHv27FGvvPKKcnV1NbjytTz6dPDgQTVu3Di1detWlZqaqubNm6dq1qypWrdubbJ9euutt9Tq1atVamqq2rlzp3rrrbeURqNR//zzj1LK/D6jO+mXOX5ON3Pt1evm+nndjUof1L1791Z+fn7KxsZGVa1aVfXu3VsdPHhQv/zy5ctqwIABys3NTTk4OKgePXqo9PR0g20cOXJEde7cWdnb2ytPT0/1+uuvq4KCAoN1Vq5cqaKiopSNjY2qWbOmmj59ern1YeXKlQq4burbt69SSlei9d577ykfHx9la2urYmJiVEpKisE2zp49q5566ilVpUoV5ezsrJ5//nmVk5NjsE5ycrJq2bKlsrW1VVWrVlUfffTRdW357bffVK1atZSNjY2KiIhQCxcuLPc+Xbp0SXXo0EF5eXkpa2trFRgYqF5++eXr/kGYWp9u1B/A4LtwP79vU6dOVQEBAcrGxkZFR0erTZs2lXufjh07plq3bq3c3d2Vra2tCgkJUcOHDzeozzW1Pr3wwgsqMDBQ2djYKC8vLxUTE6MPaaXM7zO6k36Z4+d0M9cGtbl+XndDo5RSFb/fLoQQQoiyeODOUQshhBDmRIJaCCGEMGES1EIIIYQJk6AWQgghTJgEtRBCCGHCJKiFEEIIEyZBfQv5+fmMGTPmpnf2MUfSJ/NRGfslfTIPlbFPYL79kjrqW8jOzsbFxYULFy7g7Oxs7OaUC+mT+aiM/ZI+mYfK2Ccw337JHrUQQghhwiSohRBCCBNm1uNRFxYWsmPHDnx8fLCwKP+/OXJycgA4efIk2dnZ5b59Y5A+mY/K2C/pk3mojH0C0+qXVqvl1KlTNGjQACurW0exWZ+jTkxMJDo62tjNEEIIIcpky5YtNGnS5JbrmPUetY+PD6DraMmYqkIIIYSpS09PJzo6Wp9jt2LWQV1yuNvPz49q1aoZuTVCCCHE3bmT07ZyMZkQQghhwiSohRBCCBMmQS2EEEKYMLM+Ry2EEOWtqKiIgoICYzdDmDlra2ssLS3LZVsS1KUVFUDmXvCsBdZ2xm6NEOI+UkqRkZFBVlaWsZsiKglXV1d8fX3RaDT3tB0J6tLO7IdvWoHGAtyCwCscvMN1//cKB89QsLY3diuFEBWgJKS9vb1xcHC451+u4sGllOLSpUtkZmYC3HP5sAR1abmZYOcKeVlw7pBuSll4dbnGAtxqXA3ukiD3qQsW5XOIQwhx/xUVFelD2sPDw9jNEZWAvb1upy4zMxNvb+97OgwuQV1acDsYcUQX2Kf3XZ0y98HpvXD5PJw7rJtSFumeo7GEkelXg3rPPCi4DEGtwdnfaF0RQty5knPSDg4ORm6JqExKvk8FBQUS1OVBKcWPG4/SpZ4fXk4+4OQDNduUXgEuni4V3MWTtgisbK+ut/5zOLkVHp8BET10805ugz0JV/fAPWuBjeN97Z8Q4vbkcLcoT+X1fZKgLrbx8FlGJ/zL+MV7eaZpIK+0qYm3U6kLyjQaqOKtm4Ja33xDgc3Byk53OLzEkXWwfkqplTTgGnCDc+C1wLZKeXdNCCGEGZM66mK2VhZEVnclr0DL9+tSaTVhJePm7yEzO+/uNtThfXh+oe7CsxJ+UdDkJajRChw8AQVZR+HAElj/GcztD9+1g/FVYUo9mPk4rJ1cnt0TQog7VqNGDaZMmXLH669atQqNRlPhV8zPmDEDV1fXCn0NUyR71MUaBbozd0BzVu8/zWfLD7DjWBY/rE/l581HeTo6gFfbBOPrUsaSrZptDA+jXzxTfAh9L5xOufrzpTOQdUw3FV2BVsOuPuf7WLB3g0c+BZfi+5orpdvTF0I8kG53aHX06NGMGTPmrrebmJiIo+Odn55r3rw56enpuLi43PVriduToC5Fo9HQNsybNrW8WHvgDJ8tP8C2o+eZseEIv2w+xpPR1Xm1TTD+rvdYouXoCY4toUZLw/klAX56X/Ged7HLWXAiUfezrdPV+X+/BfsWgVdYqUPotcGrluF6QohKKT09Xf/zr7/+yqhRo0hJSdHPq1Ll6qk0pRRFRUW3HfsYwMvL667aYWNjg6+v7109R9w5OfR9AxqNhta1vPjj1WbMfKkp0TXcuVKk5ceNR2k7cRUj5+ziZNbl8n9hR09deDd5CSK6X51v7QDPL4ZHvwK7Un+xZu6BC8fg4FLYMBXmxcP37WF8Nfi0Lvz8GCwZCdt/ghNbIa/yDAAvhABfX1/95OLigkaj0T/et28fTk5OLF68mEaNGmFra8u6des4dOgQjz76KD4+PlSpUoUmTZqwbNkyg+1ee+hbo9Hw/fff06NHDxwcHAgNDSUhIUG//NpD3yWHqJcsWULt2rWpUqUKnTp1MvjDorCwkMGDB+Pq6oqHhwcjRoygb9++dO/e/a7eg2nTphEcHIyNjQ1hYWH89NNP+mVKKcaMGUNAQAC2trb4+/szePBg/fKvvvqK0NBQ7Ozs8PHxoVevXnf12veL7FHfgkajoUWIJ82DPdh4+CyfLTvA5tRzzNx8jN+2HqdXo+oMaBtMdfcKLumwstFdpBbY3HD+4/+7poSseMo9BReO66aDhv8AeSgeOn2o+7moANKSdHvgdnLISojSlFJcLigyymvbW1uW2xXDb731FpMmTaJmzZq4ublx/PhxunTpwgcffICtrS0//vgjcXFxpKSkEBAQcNPtjB07lo8//piJEycydepU+vTpw9GjR3F3d7/h+pcuXWLSpEn89NNPWFhY8Mwzz/DGG28wc+ZMACZMmMDMmTOZPn06tWvX5rPPPmPu3Lm0a9fujvs2Z84chgwZwpQpU4iNjWXBggU8//zzVKtWjXbt2vHnn3/y6aefMnv2bCIiIsjIyCA5ORmArVu3MnjwYH766SeaN2/OuXPnWLt27V28s/ePBPUd0Gg0NA/2pHmwJ5uKA3vj4bPM2nKM37cep1ejagxoG0KAx32uwXRwv3GAXzp39dx36SDPzQDnUnfIOXMA/hsLti7w1tGr57sPrdSVj3mFSYCLB9blgiLqjFpilNfeM64jDjbl8+t53LhxPPzww/rH7u7uREZG6h+///77zJkzh4SEBAYOHHjT7fTr14+nnnoKgA8//JDPP/+cLVu20KlTpxuuX1BQwNdff01wcDAAAwcOZNy4cfrlU6dO5e2336ZHD10Z6xdffMGiRYvuqm+TJk2iX79+DBgwAIBhw4axadMmJk2aRLt27Th27Bi+vr7ExsZibW1NQEAA0dHRABw7dgxHR0ceeeQRnJycCAwMpEGDBnf1+veLBPVdeqimBw+94sGW1HN8vvwA6w6eYXbicX7fdoKeDaoysH0IgR5GrpF2cIfAZrqptMvngVJ/pV86C05+4FLd8KK0RW/A2YO6n538i8+B19b936v4//auFd0LIUQ5aNy4scHj3NxcxowZw8KFC0lPT6ewsJDLly9z7NixW26nfv36+p8dHR1xdnbW3yLzRhwcHPQhDbrbaJasf+HCBU6dOqUPTQBLS0saNWqEVqu9477t3buXV155xWBeixYt+OyzzwB4/PHHmTJlCjVr1qRTp0506dKFuLg4rKysePjhhwkMDNQv69Spk/7QvqmRoC6j6CB3fn6pKduOnmPKsgOsPXCG37ed4K8dJ+kepQvsIE8Tu6mJvZvh46BW8Po+KMy/Ok8p3X3Or1yCnLSr0+GVhs918jMM7pAYXW24EJWEvbUle8Z1NNprl5drr95+4403WLp0KZMmTSIkJAR7e3t69erFlStXbrkda2trg8cajeaWoXqj9ZVSd9n6e1O9enVSUlJYtmwZS5cuZcCAAUycOJHVq1fj5OTE9u3bWbVqFf/88w+jRo1izJgxJCYmmlwJmAT1PWoU6M5PLzZl+7HzfL78AKtSTvPn9hPM2XGCR4sDO9jLxG9iUvrOahoNPPOH7ufLWbqBSjL3Gh5Cz0mDnHTddHiVbt1e068G9YltsOs33SH5Oo/ez54IUW40Gk25HX42JevXr6dfv376Q865ubkcOXLkvrbBxcUFHx8fEhMTad1adwOpoqIitm/fTlRU1B1vp3bt2qxfv56+ffvq561fv546deroH9vb2xMXF0dcXBzx8fGEh4eza9cuGjZsiJWVFbGxscTGxjJ69GhcXV1ZsWIFPXv2LLe+lofK9y00koYBbsx4Ppqk41l8vvwAK/ZlMmfHSeYlnSQu0p9B7UMI8Tazkil7V6gerZtKy7tQqv67+D7ovvWuLj+6HjZ/DTkZV4Naq4VZvcG9puGAJtfu5QshKlRoaCh//fUXcXFxaDQa3nvvvbs63FxeBg0axPjx4wkJCSE8PJypU6dy/vz5u7qIbvjw4TzxxBM0aNCA2NhY5s+fz19//aW/in3GjBkUFRXRtGlTHBwc+Pnnn7G3tycwMJAFCxZw+PBhWrdujZubG4sWLUKr1RIWFlZRXS4zCepyFlXdlR/6NWHnCV1gL9ubybykNBKS03ikvj+D24cQ6mNmgX0tO5cbB3iJao2h2UDdHdlKXDgOB/65ft0qPobBXfKzw42vJBVC3JvJkyfzwgsv0Lx5czw9PRkxYgTZ2fe/dHPEiBFkZGTw3HPPYWlpySuvvELHjh3vavCK7t2789lnnzFp0iSGDBlCUFAQ06dPp23btoBuPOiPPvqIYcOGUVRURL169Zg/fz4eHh64urry119/MWbMGPLy8ggNDWXWrFlERERUUI/LTqPu90mDcnTixAmqV6/O8ePHqVatmrGbc0O7T17g8+UH+GfPKUB3ZLlLXT8GxYQQ7uts5NbdR5fPw94Fpa5ET9GF9804euuCu9sX4Baom1dUCJbyt6Uof3l5eaSmphIUFISdXRnvQCjuiVarpXbt2jzxxBO8//77xm5OubjV9+pu8kt+61WwulVd+Pa5xvybdoGpyw/y978ZLNyVzsJd6XSu68vgmFBq+z0AgW3vBg2fNZyXnwOn9+sOnesPo6fobuJyMRNSMw3Lw/55F3b/AW1GQPTLunmF+ZCfC44yhrAQ5uTo0aP8888/tGnThvz8fL744gtSU1N5+umnjd00kyNBfZ9E+Lvw9bON2JeRzdTlB1m0O53FuzNYvDuDDnV8GBwTSt2qD1jNsq0TVGukm0rLz9FdxHYu1bAM7PRe3VCjVqX+Mj25DaZ31t1yVV9CVnIovbbubm9CCJNjYWHBjBkzeOONN1BKUbduXZYtW0bt2rWN3TSTI0F9n4X7OvNln4bsP5XD58sPsHBXOv/sOcU/e04RW9uHITGh1Kv2gAX2tWydoGoj3VRa75m6AHepfnVeVnHt56UzcGStbirNweNqCVnpWnBHTxnQRAgjql69OuvXrzd2M8yCnKM2sgOncpi64iDzd6ZR8knEhHszOCaUyOquRm2b2bhysfgq9JTiw+gpupKyrKM3Xt/GCd4+fjWo9/8D1na6i9/sHoDTEOI6co5aVAQ5R11JhPo48flTDRgcE8qXKw8yL+kky/dlsnxfJm3DvBgSE0qDAClhuiUbR6jaUDeVduVicR34PsPbqTp6G+5NL3kHzh6AZ/7S3bgFdIfUT2y7uifu6CV74EIIo5CgNhEh3lX4tHcUg9qH8MXKg8xLSmNVymlWpZymdS1dYDcKlMC+KzaO4N9AN5VWVHj1Z6V0V5drC3WBXGLfIlg76epje/frS8i8wqGKtwS4EKJCSVCbmJpeVZj8RBSD2+v2sP/acZI1+0+zZv9pWoZ4MiQ2lCY1pMb4npQu8dJooPfP16/jFQZhXXR74OdS4fI5OLZBN5Vm71Yc2mEQ2BLqP16xbRdCPHAkqE1UDU9HJj4eyaDiwP5z+wnWHTzDuoNnaB7swZCYUJrWlJKkClP/Cd0EUHBZN9KYwZCie+H8EV19+LGNuuny+atBrdXCT93BPQgeHiejkAkhykyC2sQFeDgwoVd9BrYP4atVB/l96wk2HDrLhkNneaimO0NiatEsWAK7Qlnbg1993VSaPsCLb6da+tB59glIXQ1HN0CXUofQFw2H9OTr78bm5CeH0IUQNyRBbSaquzswvmd94tuFMG3VIX7bepxNh8+x6fAmooPcGRITSvNgj3IbbF7cgZsFOICdKzz2X13dt2WpUYSOb4H0JDi+2XB9W5fiC9euOQfu7C8BLipc27ZtiYqKYsqUKQDUqFGDoUOHMnTo0Js+R6PRMGfOHLp3735Pr11e27mVMWPGMHfuXJKSkirsNSqSBLWZqebmwAc96ukD+9fE42xJPUef7zfTONCNIbGhtAzxlMA2NjtnqNfr+vk9voZT/xoeRj93GPIvwIktuqk0W2do+zY0G6B7XJivC3/nqhLggri4OAoKCvj777+vW7Z27Vpat25NcnKywVjSdyIxMfG64THv1c3CMj09HTc3uVD2ViSozZS/qz3vd6/LgHbBfLP6ML9sOcbWo+d59r9baBjgyuCYUNrU8pLANjXetQ0PkYMufM8eLB5OtFQt+NlDkJ8NtqWGST25HaZ30t20JX7T1fkntoGTjwT4A+bFF1/kscce48SJE9fV4k6fPp3GjRvfdUgDeHl5lVcTb8vX1/e+vZa5sjB2A8S98XOxZ0y3CNa+2Y7nW9TA1sqC7cey6Dc9kR5fbWDlvsz7Pli7uEtWtuATodsDbz9SdxX6wEQYmQ79N+iuPi+RfRIsrMClquE2Zj0Jn0bA+OrwXQzMi4cNU+HAMsg6DvIdqJQeeeQRvLy8mDFjhsH83Nxcfv/9d1588UXOnj3LU089RdWqVXFwcKBevXrMmjXrltutUaOG/jA4wIEDB2jdujV2dnbUqVOHpUuXXvecESNGUKtWLRwcHKhZsybvvfceBQUFgG64ybFjx5KcnIxGo0Gj0ejbrNFomDt3rn47u3bton379tjb2+Ph4cErr7xCbm6ufnm/fv3o3r07kyZNws/PDw8PD+Lj4/WvdSe0Wi3jxo2jWrVq2NraEhUVZXBU4sqVKwwcOBA/Pz/s7OwIDAxk/PjxACilGDNmDAEBAdja2uLv78/gwYPv+LXLQvaoKwkfZztGx0XQv00w36w5zMzNR0k6nsXzMxKJrObC4JhQ2od7yx62OSkJ8NLq9YLa3XRjgpe4cklXJnb5HFzJgZNbdVNpNlWu3j615CYu1RrLeOB34srFu3+Ope3VMsCiQijKB42F7rqG223X5s4POVtZWfHcc88xY8YMRo4cqf/3/fvvv1NUVMRTTz1Fbm4ujRo1YsSIETg7O7Nw4UKeffZZgoODiY6+yVC1pWi1Wnr27ImPjw+bN2/mwoUL15+7VgqnKlWYMf0H/P382bVrJy//51Wcqjjy5vDh9O7dm927d/P333+z7O9FALi4l7oPv7YQCi5z8eJFOnbsQLOmTUncsJrMzNO89OpABvb/DzO+/wZQUFTAypUr8fP2ZOXKlRw8eJDevXsTFRHGy88/qztdZFE8VOaVS1B4Wfd/baHutBHw2dSv+eSTSXzz2UQa1K/LDz/Nolu3bvy7eRWhtUL5fNr/SEhI4LfffiPAw57jx45z/EwOAH/++Seffvops2fPJiIigoyMDJKTk+/4MysLCepKxtvZjvceqcOrbYL5ds0hft50jOQTF3jxf1upW9WZwe1DebiOjwS2ObOygSqlDk3aOMDALVB4Bc4dKlVCVjydPQhXcnV3Wzu57erznvkTQmJ1P5/YCkfXQ0Czm48zbixKgdLqgq7ke1t4RfcL2MLqarApBTnpunW1Rbr/l0z6x0WlHivdY/dgsCzeRlEB5F3R/aIv2e6H/nff5rjPdeV91vawbz783g+qRcNTv+jucgcwpR5cOnv9c1/bDQqK/6P7v/6AiNJdqFgyWE3hFV7o+TATJ05k9erV+nGYp3/3NY898jAuBZm42Cje6NuteHsFDOr9MEsSWvLbjGlE13DSvQ9XLl3/h0N2OmTsYtnOdPbt28eSJUvwt78CefDh6y/S+ZmBuusr0nYA8O6LjxQ/8QI1mgTyxitPMfuXH3nz5d7YuwdRpUoVrKys8LUo7rNtqff1chac3scvM/8i7/Ilfpz4Jo4O1uDjzxfjhhHXbygT3ngeHy8PyM/BzdmRL95/A0ufcMLDw+natSvLlyzi5e6tdBdhWhT/QZR3AXIzdNeAFBXAhRMATPpsKiP6P8eTHXTf9QnDX2TlqlVM+WwKX348lmPHjhEaGkrLli3RnN5HoGsN8AgB4NixY/j6+hIbG4u1tTUBAQF39AfPvZCgrqS8nGwZ2bUO/2kTzHdrD/PTxqPsPpnNKz9to46fM4NjQulQxwcLCwnsSsPK5uo58NI74kUFuvPdpW+jmrlPt3ddYv/fsGYiNOx7NagL8uCPF0oFXtGdheDjM8C9pm4bm7+FLd9A3ceg3Tu6eZfOwbTmNwjPa16nZNslKdV3AQS10v28/X+w6A2o8yg88ePVfkwuw8hLj8+A4M7Ffb4EF9J0RyA8Q+9+WyUun9P9cVR6D7ooHy6euRrUN3PxzK2XW9peDWpVRHiAJ80bR/HDDz/Qtm1bDh48yNqNWxj3+7dwJZeioiI+/PwHfluwlJMZmVy5UkD+lQIc7GygMK94o8Xvd2mqCLSF7N2zh+rVq+Pv76+7+Y/S0qxRveua9eu8JXz+w2wOHT1B7sVLFBYV4VzFkVJ/ZRQr+Z1Tar5GAxZW7D14lMg6YThWcdbPb/FQE7RaLSmpafj4VQULSyLCQ7G0vfre+vn5sWv7cd0fV5pSZ3StbHV72Ja2uj++7FzIzs4hLeM0LVq0LD6ipGtPi2ZNSd69F+xc6devHw8//DBhYWF0at+aRzq2p0Oc7rv1+OOPM2XKFGrWrEmnTp3o0qULcXFxWFlVXJxKUFdynlVsebtzbf7TWhfYP244wp70bF79eRvhvk4MjgmlU4SvBHZlZmmtK/vyDr/5Oj4RENETarS8Ou/MfkhZePevV3D56s+Xz+v26HMzDdfJSb/77ZYOkpJfxgbzNGBpA2h0yy0sr+6FayyvmWdRPE8DVqXCVGOpG0bV0ubqvHfSigOqiKshoyn+sdS/G02pZZbWunAACI+D4Yfh8lndgDAlhu7SvS/aQv3TQAPWDjfYvuaa5cUsrcGlOi++8AKD3niLL7/8kunTpxNcM4g2XR4DjYaJk6bw2fRfmTJxPPUi6uDo6MjQ4W9zBSv9XiJW9obbBd098b3CgdVX57lU05UM2hafenGtDj512bhxE30GvcvYMaPp2KEDLi4uzP71Nz6ZPBncggy36x/FdRw8wLeeblQ7G0fwrXt1mV3xa7nX0P0RaueCtaMC18BSb70GraUteNa6ZrvuVydLG90fkFbZumXOfuBW4+q6ts66z97Zj4YN/UhNTWXx4sUsW7aMJ54fQGzsX/zxxx9Ur16dlJQUli1bxtKlSxkwYID+iIa1tTUVwehBffLkSUaMGMHixYu5dOkSISEh+qsVRflxd7RhRKdwXmlVk/+uS2XGhiPsy8hhwMzthPk4MSgmhC51/SSwH1QRPXRTaQ7u8MinNwi54oC7bl7x49LDkEY9DUGtdfdEL2HrDP9ZezUwDcLz2nmllpUe2axhX2jwjG55ae+dLlv/84r3LG2rgMs1Y5hfGxx3y9IKHD1007XbdQ+68XPulIUVOHryxLPPM+TNd/jll1/48ccf6d+/PxoH3a2G12/ZzqOPdueZ518BdOec9x88TJ06dXRDyoLu/ba45r20sgFre2rXqcPx48dJT0/Hz88PgE1bdxQ/zxosrdmweQuBgYGMfPc9/dOPHisegrb4DxgbGxuKiopu2Z3atWszY8YMLl68qC8PW79+PRYWFoSFhZX5bSrN2dkZf39/1q9fT5s2bfTz169fb3AI29nZmd69e9O7d2969epFp06dOHfuHO7u7tjb2xMXF0dcXBzx8fGEh4eza9cuGjZseKOXvGdGDerz58/TokUL2rVrx+LFi/Hy8uLAgQNSU1eB3BxteKNjGC+1CuKHdalMX3+ElFM5DPxlB6HeBxgUE0rXen5YSmALl2rQ+IV724Zrdd1UmqXVjW8SczcsrTCB/QyTUaVKFXr37s3bb79NdnY2/fr10y8LDQ3ljz/+YMOGDbi5uTF58mROnTqlC+o7EBsbS61atejbty8TJ04kOzubkSNHGqwTGhrKsWPHmD17Nk2aNGHhwoXMmTPHYJ0aNWqQmppKUlIS1apVw8nJCVtbW4N1+vTpw+jRo+nbty9jxozh9OnTDBo0iGeffRYfH5+yvTk3MHz4cEaPHk1wcDBRUVFMnz6dpKQkZs6cCcDkyZPx8/OjQYMGWFhY8Pvvv+Pr64urqyszZsygqKiIpk2b4uDgwM8//4y9vT2BgYG3edWyM2p51oQJE6hevTrTp08nOjqaoKAgOnToQHBwsDGb9UBwdbBhWIcw1o1oz5CYUJzsrDiQmcvgWTvo8Olq5u44SZFWSnqEMBcvvvgi58+fp2PHjrrzycXeffddGjZsSMeOHWnbti2+vr53dRcwCwsL5syZw+XLl4mOjuall17igw8+MFinW7duvPbaawwcOJCoqCg2bNjAe++9Z7DOY489RqdOnWjXrh1eXl43LBFzcHBgyZIlnDt3jiZNmtCrVy9iYmL44osv7u7NuI3BgwczbNgwXn/9derVq8fff/9NQkICoaG66xKcnJz4+OOPady4MU2aNOHIkSMsWrQICwsLXF1d+e6772jRogX169dn2bJlzJ8/Hw+PiruVs0YZsci2Tp06dOzYkRMnTrB69WqqVq3KgAEDePnll+/o+Xcz8La4tey8AmasP8J/16Vy4bKuHrGmpyMD24fQLdIfK0spuReVV15eHqmpqQQFBWFnZ2fs5ohK4lbfq7vJL6P+9j18+DDTpk0jNDSUJUuW0L9/fwYPHsz//ve/G66fn59Pdna2fsrJybnPLa68nO2sGRwTyroR7XijQy1cHaw5fOYiw35LJnbyav7YdoLCIu3tNySEEKJcGXWP2sbGhsaNG7Nhw9UxfgcPHkxiYiIbN268bv0xY8YwduzY6+bLHnX5y80v5MeNR/huzWHOX9LtYQe4OzCwXQg9GlbFWvawRSUie9SiIlSKPWo/P7/rLmioXbs2x0quFrzG22+/zYULF/TTnj177kczH0hVbK0Y0DaEdSPa81bncDwcbTh27hJv/rmT9p+sYvaWY1wplD1sIYSoaEYN6hYtWpCSkmIwb//+/Te9es7W1hZnZ2f95OTkdMP1RPlxtLXi1TbBrB3Rjne6hONZxYbj5y7z1l+7aDdpFb9slsAWQoiKZNSgfu2119i0aRMffvghBw8e5JdffuHbb78lPj7emM0SN+BgY8UrrYNZ+2Z73u1aGy8nW05mXeadObtoO3ElP206Sn7hrWskhRBC3D2jBnWTJk2YM2cOs2bNom7durz//vtMmTKFPn36GLNZ4hbsbSx5qVVN1r7ZjlGP1MHbyZa0C3m8N3c3bSeu4seNR8grkMAW5kmrlaNDovyU1/fJqBeT3SspzzK+vIIifk08zrRVh8jI1t3dycfZllfbBPNUdAB21pa32YIQxqfVajlw4ACWlpZ4eXlhY2MjA9eIMlNKceXKFU6fPk1RURGhoaFYWBjuF99NfklQi3KRV1DE71uP89WqQ6Rf0AW2t5Mt/2kTTJ+mEtjC9F25coX09HQuXbpk7KaISsLBwQE/Pz9sbGyuWyZBLYwmv7CI37eeYNqqQ5zM0g3O4FnFllfb1OTppgE42MhtH4XpUkpRWFh423tSC3E7lpaWWFlZ3fTIjAS1MLorhVr+3H6CL1ce5MT5ksC24eVWNXm2WaAEthDigWY2ddSi8rKxsuCp6ABWvtGWCY/Vo7q7PWdyrzB+8T5aTljJtFWHuJhfaOxmCiGEyZOgFhXK2tKC3k0CWPF6Wyb2qk+ghwPnLl5hwt/7aDlhBV+uPEhOXoGxmymEECZLglrcF9aWFjzeuDrLh7Xhk8cjCfJ05PylAiYuSaHVxyuZuvwA2RLYQghxHQlqcV9ZWVrwWKNqLH2tNZ/2jqSmlyNZlwr4ZOl+Wn60gs+WHdCP3iWEEEKCWhiJlaUFPRpUY+lrbfjsySiCvRzJzivk02X7aTlhBZOX7ufCJQlsIYSQoBZGZWmh4dGoqvzzWhumPtWAWj5VyMkr5PPlB2g5YQWf/JNC1qUrxm6mEEIYjQS1MAmWFhriIv35e0hrvny6IWE+TuTkFzJ1xUFaTljJxCX7OH9RAlsI8eCRoBYmxcJCQ9f6fiwe0oqvn2lIbT9ncvML+XLlIVpOWMFHi/dxNjff2M0UQoj7RoJamCQLCw2d6vqxcFBLvnm2ERH+zly8UsTXqw/R6uOVjF+0lzMS2EKIB4AEtTBpFhYaOkb4smBQS75/rjH1qrpw6UoR36w5TMsJK/i/BXvIzMkzdjOFEKLCSFALs6DRaIit40PCwBb80K8xkdVcyCvQ8v26VFpNWMm4+XvIzJbAFkJUPhLUwqxoNBrah/swN74F059vQlR1V/ILtfywPpVWH69kTMK/nJLAFkJUIhLUwixpNBrahXkzZ0BzfnwhmkaBbuQXapmx4QitPl7J6Hm7Sb9w2djNFEKIeyZDGAmzptFoaF3Li1ahnqw/eJbPlu8n8ch5/rfxKLO2HOeJJtXo3zaEqq72xm6qEEKUiQS1qBQ0Gg0tQz1pEeLBxsNn+WzZATannuPnTcf4NfE4jzeuzoC2wVRzczB2U4UQ4q5IUItKRaPR0DzYk+bBnmwqDuyNh8/yy+Zj/JZ4nF6NqhHfLoTq7hLYQgjzIOeoRaX1UE0PZr3yEL/9pxktQjwo1CpmJx6n3aRVvPlHMsfOXjJ2E4UQ4rYkqEWlFx3kzsyXHuKPV5vRKtSTQq3it60naPfJKt74PZkjZy4au4lCCHFTEtTigdG4hjs/vdiUP/s3p00tL4q0ij+2naD9J6sY9msSh0/nGruJQghxHQlq8cBpFOjG/16IZm58C9qFeaFV8NeOk8ROXs3Q2Ts4mCmBLYQwHRLU4oEVVd2V6c9HkzCwBbG1vdEqmJuUxsOfrmbQrB0cOJVj7CYKIYQEtRD1q7nyfd8mLBjUkofr+KAUzE9Oo8OUNcT/sp2UDAlsIYTxSFALUaxuVRe+e64xCwe3pFOEL0rBwp3pdJyyhgEzt7E3PdvYTRRCPIAkqIW4RoS/C18/24jFQ1rRpZ4vAIt2ZdD5s7X856et/Jt2wcgtFEI8SCSohbiJ2n7OfNWnEUuGtuaR+n5oNLDk31N0/XwdL/+4ld0nJbCFEBVPglqI2wjzdeKLpxvyz9DWdIv0R6OBpXtO8cjUdbw4I5GdJ7KM3UQhRCVWpqA+fvw4J06c0D/esmULQ4cO5dtvvy23hglhakJ9nPj8qQYsfa0N3aP8sdDA8n2ZdPtiPc9P30LS8SxjN1EIUQmVKaiffvppVq5cCUBGRgYPP/wwW7ZsYeTIkYwbN65cGyiEqQnxrsKUJxuwbFgbejasioUGVqacpvuX6+n7wxa2Hztv7CYKISqRMgX17t27iY6OBuC3336jbt26bNiwgZkzZzJjxozybJ8QJqumVxUmPxHFitfb0qtRNSwtNKzef5qeX23g2f9uZuuRc8ZuohCiEihTUBcUFGBrawvAsmXL6NatGwDh4eGkp6eXX+uEMAM1PB2Z9HgkK15vQ+/G1bGy0LD2wBl6fb2RPt9vYkuqBLYQouzKFNQRERF8/fXXrF27lqVLl9KpUycA0tLS8PDwKNcGCmEuAj0cmdCrPivfaMtT0brAXn/wLE98s5Env93IxkNnjd1EIYQZKlNQT5gwgW+++Ya2bdvy1FNPERkZCUBCQoL+kLgQD6rq7g6M71mfVcPb8nTTAKwtNWw6fI6nvtvEE99sZMPBMyiljN1MIYSZ0Kgy/sYoKioiOzsbNzc3/bwjR47g4OCAt7d3uTXwVk6cOEH16tU5fvw41apVuy+vKcTdOpl1ma9XHeLXxONcKdIC0KSGG0NiatEixAONRmPkFgoh7re7ya8y7VFfvnyZ/Px8fUgfPXqUKVOmkJKSct9CWghzUdXVnve712X1m23p2ywQGysLEo+c55n/bqbX1xtZvf+07GELIW6qTEH96KOP8uOPPwKQlZVF06ZN+eSTT+jevTvTpk0r1wYKUVn4udgz9tG6rH2zHc+3qIGtlQXbjp6n7w9b6PHVBlamZEpgCyGuU6ag3r59O61atQLgjz/+wMfHh6NHj/Ljjz/y+eefl2sDhahsfJztGB0Xwdo32/FiyyDsrC1IOp7F89MT6f7lepbvPSWBLYTQK1NQX7p0CScnJwD++ecfevbsiYWFBQ899BBHjx4t1wYKUVl5O9vx3iN1WPNmO15upQvs5BMXePF/W+n2xXqW7pHAFkKUMahDQkKYO3cux48fZ8mSJXTo0AGAzMxMnJ2dy7WBQlR23k52jOxah3Uj2vOf1jWxt7Zk18kLvPzjVh6Zuo4l/2ZIYAvxACtTUI8aNYo33niDGjVqEB0dTbNmzQDd3nWDBg3K1JCPPvoIjUbD0KFDy/R8IcydZxVb3u5Sm3Uj2tG/bTCONpb8m5bNf37aRpfP17F4VzparQS2EA+aMpdnZWRkkJ6eTmRkJBYWurzfsmULzs7OhIeH39W2EhMTeeKJJ3B2dqZdu3ZMmTLljp4n5VmiMjt/8QrfrzvM/zYcJTe/EIBwXycGtQ+lc11fLCykrEsIc1Xh5VkAvr6+NGjQgLS0NP1IWtHR0Xcd0rm5ufTp04fvvvvOoCZbiAedm6MNwzuGs25EOwa3D8HJ1op9GTnE/7KdjlPWkJCcRpHsYQtR6ZUpqLVaLePGjcPFxYXAwEACAwNxdXXl/fffR6vV3tW24uPj6dq1K7GxsbddNz8/n+zsbP2Uk5NTluYLYVZcHWwY1iGMdSPaMyQmFCc7Kw5k5jJ41g46TlnDvKSTEthCVGJWZXnSyJEj+e9//8tHH31EixYtAFi3bh1jxowhLy+PDz744I62M3v2bLZv305iYuIdrT9+/HjGjh1bliYLYfZcHKx57eFavNAyiBnrj/DfdYc5mJnLkNlJfLb8AIPahxBX3x8ryzIfKBNCmKAynaP29/fn66+/1o+aVWLevHkMGDCAkydP3nYbx48fp3HjxixdupT69esD0LZtW6Kiom56jjo/P5/8/Hz945MnT1KnTh05Ry0eSDl5BfxvwxG+X5dK1qUCAII8HRnYLoRHoySwhTBld3OOukxBbWdnx86dO6lVq5bB/JSUFKKiorh8+fJttzF37lx69OiBpaWlfl5RUREajQYLCwvy8/MNlt2IXEwmBOTmF+oCe+1hzhcHdqCHA/HtQujRoCrWEthCmJwKv5gsMjKSL7744rr5X3zxhX7v+HZiYmLYtWsXSUlJ+qlx48b06dOHpKSk24a0EEKniq0V8e1CWDeiPW91Dsfd0YajZy/x5h87af/JKn5NPEZB0d1dOyKEMB1lOkf98ccf07VrV5YtW6avod64cSPHjx9n0aJFd7QNJycn6tatazDP0dERDw+P6+YLIW7P0daKV9sE81yzQH7edJRv1xzm+LnLjPhzF58vP0h8uxB6NaqGjZXsYQthTsr0L7ZNmzbs37+fHj16kJWVRVZWFj179uTff//lp59+Ku82CiHugoONFa+0Dmbtm+15t2ttPKvYcjLrMu/M2UW7Sav4edNR8guLjN1MIcQdKvMNT24kOTmZhg0bUlR0f34JyDlqIW7v8pUiZm05xterD5GZo7sY08/FjgFtg3m8cXXsrOU0kxD323254YkQwjzY21jyQssg1rzZjjFxdfBxtiX9Qh7vzfuXthNXMWN9KnkFsocthKmSoBbiAWFnbUm/FkGsHt6O9x+NwM/FjozsPMbM30Prj1fywzoJbCFMkQS1EA8YO2tLnm1Wg1XD2/J/3evi72JHZk4+4xbsoeWElXy/9jCXr0hgC2Eq7uqq7549e95yeVZW1r20RQhxH9laWfLMQ4E80bg6f2w7wZcrD3Iy6zL/t3AvX68+xCuta/LMQ4E42JSpOEQIUU7u6l+gi4vLbZc/99xz99QgIcT9ZWNlwdNNA+jVqBpzdpzgi5UHOX7uMh8u2sfUFQfpXNeXbpFVaRbsgaWM2CXEfVeuV33fb3LVtxDlr6BIy5wdJ/ly5UGOnr2kn+9ZxZZH6vsRF+lPwwBXNBoJbSHKqsJvIWoqJKiFqDharWLLkXMkJKexaFe6/n7iANXc7ImL9KdbpD/hvk4S2kLcJQlqIUS5KijSsu7AGRKS01jybwaXSl1sFupdhW6R/nSL8ifQw9GIrRTCfEhQCyEqzOUrRazYl0lC8klW7jvNlVL3EY+s5kJcpD9xkf74ONsZsZVCmDYJaiHEfZGdV8CS3RkkJKex/uAZtMW/TTQaaBrkTrfIqnSu64ubo41xGyqEiZGgFkLcd2dy81m0K515SWlsO3peP9/KQkPrWl50i/Tn4To+ONpKuZcQd5Nf8i9GCFEuPKvY8lyzGjzXrAYnzl9ifnI6Cclp7E3PZsW+TFbsy8TO2oKY2j50i/SnbZgXtlZyn3Ehbkf2qIUQFepgZg4JSWkkJKdxpFS5l5OdFZ0ifOkW5U+zmh5YWcqNEsWDQw59CyFMjlKK3SezmZd0kgU708nIztMv86xiQ9d6fnSL8qdhgJuUe4lKT4JaCGHSStdoL96VzvlSNdpVXa/WaNf2kxptUTlJUAshzEbpGu1//s3gYqka7ZCSGu1If2p4So22qDwkqIUQZimvoIjle4trtFNOc6Xwao12/WoudIv055H6/vi6SI22MG8S1EIIs1e6RnvDobMUFRdpazQQXcOdblH+dKnrJzXawixJUAshKpWSGu2EpDS2XlOj3SrUk25R/jxcx5cqUqMtzIQEtRCi0jpx/hILdupCe096tn6+nbUFMeE+xBXXaNtZS422MF0S1EKIB8LBzFwSktOYn5xG6pmL+vlOdlZ0jPDlUanRFiZKgloI8UApqdFOSD7J/OTra7S71POjW6SuRtvCQsq9hPFJUAshHlharSKx1Dja19ZoPxKpC+06fs5Soy2MRoJaCCEortE+eIb5SbpxtEvXaAd7OfJoVFWp0RZGIUEthBDXyCsoHkc7KY0VKZnX1WjH1ffnkUg//FzsjdhK8aCQoBZCiFvIzivgn39P6cfRLl2j3aSGO90i/elSzw93qdEWFUSCWggh7tCZ3HwW79INyZl4RGq0xf0hQS2EEGVwMusyC5J1Q3L+m3a1RtvWyoLY2lKjLcqPBLUQQtyjQ6dzSUjS1WgfLl2jbWtFx7q+dIv0p3mw1GiLspGgFkKIcqKU4t+0bP2NVdIvSI22uHcS1EIIUQG0WsXWo+dJSD7Jol0ZnLt4Rb9MarTF3ZCgFkKIClZQpGX9wZJxtE+Rm1+oXxbs5Ui3yKp0i/InSGq0xQ1IUAshxH2UV1DEyn2ZJCSnsXyfYY12varF42hLjbYoRYJaCCGMJKdUjfY6qdEWNyFBLYQQJuBsbj6LdmcwPymNLUfO6edbWWhoGepJt0h/OkRIjfaDSIJaCCFMTFrWZRbs1NVo7z5pWKMdU9ubbpH+tA3zlhrtB4QEtRBCmLBDp3OZX3xjlcOnDWu0O0T40i3KnxZSo12pSVALIYQZKKnRnl9co51Wqkbbw7G4RjvKn0ZSo13pSFALIYSZ0WoV246dJyFJN4722VI12v4udsRF+hMX6U+Ev9RoVwYS1EIIYcYKi7SsP3SWhOJxtEvXaNf0cqRbpD/dIv2p6VXFiK0U90KCWgghKom8giJWpehqtJftNazRrlvVWVejXd8ff1ep0TYnZhPU48eP56+//mLfvn3Y29vTvHlzJkyYQFhY2B09X4JaCPEguVmNNkB0DXfiovzpUtcXjyq2RmyluBNmE9SdOnXiySefpEmTJhQWFvLOO++we/du9uzZg6Pj7W+7J0EthHhQ6Wu0k9PYknq1RtvSQkPLkJIabR+c7KyN2EpxM2YT1Nc6ffo03t7erF69mtatW992fQlqIYSA9AuXWZCczrzkk9fVaLcP19VotwuXGm1Tcjf5ZVK3w7lw4QIA7u7uN1yen59Pfn6+/nFOTs59aZcQQpgyPxd7Xm5dk5db1+Tw6VwSStVoL96dweLdGVSxtaJDhA/dIv1pEeKJtdRomw2T2aPWarV069aNrKws1q1bd8N1xowZw9ixY6+bL3vUQghh6FY12u6ONnSp50u3yKo0DpQabWMwy0Pf/fv3Z/Hixaxbt+6mjb52j/rkyZPUqVNHgloIIW7hdjXajxSXe0mN9v1jdkE9cOBA5s2bx5o1awgKCrrj58k5aiGEuDula7T/+TeDnNI12p6OxEX60y3Kn2Cp0a5QZhPUSikGDRrEnDlzWLVqFaGhoXf1fAlqIYQou9I12sv3ZpJfqkY7wl9Xox0XKTXaFcFsgnrAgAH88ssvzJs3z6B22sXFBXv7238xJKiFEKJ85OQVsHSPrkZ77QHDGu0mNdz042hLjXb5MJugvtm5kOnTp9OvX7/bPl+CWgghyt+5i1dYtCudhOQ0Eo+coyQlLC00tCiu0e4oNdr3xGyC+l5JUAshRMUqqdFOSE5j18kL+vk2Vha0D/OmW5Q/7aVG+65JUAshhCh3h0/nMj85nYTkkxwqNY52FVsrOtTxIS7Kn5ZSo31HJKiFEEJUGKUUe9KzSUhOY0FyOiezLuuXuTva0LmuL90i/WlSw11qtG9CgloIIcR9odUqth87T0JyGgt3GtZo+7nY8Uh9P7pFVqVuVanRLk2CWgghxH1XWKRlw6GzJCSnsWT39TXaJTdWCfGWGm0JaiGEEEalq9E+zfzkNJbtPWVQo13Hz5lHo/x5JNKfqg9ojbbZDsohhBCicrCztqRTXV861fUlN7+QpXsySEjS1WjvSc9mT3o24xfvo0kNN+KKa7Q9pUb7hmSPWgghxH1z7uIVFu9OJyEpjS3X1Gg3D/bQ1WjX9cW5ktdoy6FvIYQQJi/jQh4LduqG5Nx54sGq0ZagFkIIYVZSz1xkfvE42gczc/Xz9TXakf60DK08NdoS1EIIIcySUoq96TkkFI+jXbpG283Bms71/OgW6U+0mddoS1ALIYQwe0oV12gnpbFwVzpncitPjbYEtRBCiEqlsEjLxsO6cbT//jeDnLyrNdpBJeNom1GNtgS1EEKISiuvoIjV+08Xj6N9irwCwxrtblG6cbRNuUZb6qiFEEJUWnbWlnSM8KVjhK5Ge1nxONpr9p/W12h/tHgfjQPd6BZl/jXaskcthBCiUjh/8QqLd2eQkHySzammXaMth76FEEI80EpqtOcnp5F8TY12uzAvukVWJaa28Wq0JaiFEEKIYkfOXCThBjXajjaWdIjQDcl5v2u0JaiFEEKIa5TUaM/fmUZCknFrtCWohRBCiFvQ1WhnMT85jQU70zmTm69f5utcXKMd5U+9qi4VUqMtQS2EEELcoVvVaNfwcKBbpD/dovwJ8XYqt9eUoBZCCCHKIL9QN472jWq0a/s5M7BdCF3r+93z60gdtRBCCFEGtlZXa7Qv5heytFSN9t70bC7mF95+I+VMgloIIYS4AUdbK7o3qEr3BlXJuqSr0e4Y4Xvf2yFBLYQQQtyGq4MNT0UHGOW1K8fAnkIIIUQlJUEthBBCmDAJaiGEEMKESVALIYQQJkyCWgghhDBhZn3Vt1arK0RPT083ckuEEEKIO1eSWyU5ditmHdSnTp0CIDo62sgtEUIIIe7eqVOnCAi4ddmXWd9CtLCwkB07duDj44OFxb0fxc/JyaFOnTrs2bMHJ6fyu6drZSfvW9nJe1c28r6Vnbx3ZVPe75tWq+XUqVM0aNAAK6tb7zObdVCXt+zsbFxcXLhw4QLOzs7Gbo7ZkPet7OS9Kxt538pO3ruyMeb7JheTCSGEECZMgloIIYQwYRLUpdja2jJ69GhsbW2N3RSzIu9b2cl7VzbyvpWdvHdlY8z3Tc5RCyGEECZM9qiFEEIIEyZBLYQQQpgwCWohhBDChElQF/vyyy+pUaMGdnZ2NG3alC1bthi7SSZv/PjxNGnSBCcnJ7y9venevTspKSnGbpbZ+eijj9BoNAwdOtTYTTELJ0+e5JlnnsHDwwN7e3vq1avH1q1bjd0sk1ZUVMR7771HUFAQ9vb2BAcH8/777yOXKF1vzZo1xMXF4e/vj0ajYe7cuQbLlVKMGjUKPz8/7O3tiY2N5cCBAxXaJglq4Ndff2XYsGGMHj2a7du3ExkZSceOHcnMzDR200za6tWriY+PZ9OmTSxdupSCggI6dOjAxYsXjd00s5GYmMg333xD/fr1jd0Us3D+/HlatGiBtbU1ixcvZs+ePXzyySe4ubkZu2kmbcKECUybNo0vvviCvXv3MmHCBD7++GOmTp1q7KaZnIsXLxIZGcmXX355w+Uff/wxn3/+OV9//TWbN2/G0dGRjh07kpeXV3GNUkJFR0er+Ph4/eOioiLl7++vxo8fb8RWmZ/MzEwFqNWrVxu7KWYhJydHhYaGqqVLl6o2bdqoIUOGGLtJJm/EiBGqZcuWxm6G2enatat64YUXDOb17NlT9enTx0gtMg+AmjNnjv6xVqtVvr6+auLEifp5WVlZytbWVs2aNavC2vHA71FfuXKFbdu2ERsbq59nYWFBbGwsGzduNGLLzM+FCxcAcHd3N3JLzEN8fDxdu3Y1+O6JW0tISKBx48Y8/vjjeHt706BBA7777jtjN8vkNW/enOXLl7N//34AkpOTWbduHZ07dzZyy8xLamoqGRkZBv9mXVxcaNq0aYXmhVmPnlUezpw5Q1FRET4+PgbzfXx82Ldvn5FaZX60Wi1Dhw6lRYsW1K1b19jNMXmzZ89m+/btJCYmGrspZuXw4cNMmzaNYcOG8c4775CYmMjgwYOxsbGhb9++xm6eyXrrrbfIzs4mPDwcS0tLioqK+OCDD+jTp4+xm2ZWMjIyAG6YFyXLKsIDH9SifMTHx7N7927WrVtn7KaYvOPHjzNkyBCWLl2KnZ2dsZtjVrRaLY0bN+bDDz8EoEGDBuzevZuvv/5agvoWfvvtN2bOnMkvv/xCREQESUlJDB06FH9/f3nfzMADf+jb09MTS0tL/djWJU6dOoWvr6+RWmVeBg4cyIIFC1i5ciXVqlUzdnNM3rZt28jMzKRhw4ZYWVlhZWXF6tWr+fzzz7GysqKoqMjYTTRZfn5+1KlTx2Be7dq1OXbsmJFaZB6GDx/OW2+9xZNPPkm9evV49tlnee211xg/fryxm2ZWSjLhfufFAx/UNjY2NGrUiOXLl+vnabVali9fTrNmzYzYMtOnlGLgwIHMmTOHFStWEBQUZOwmmYWYmBh27dpFUlKSfmrcuDF9+vQhKSkJS0tLYzfRZLVo0eK6EsD9+/cTGBhopBaZh0uXLmFhYfjr3tLSEq1Wa6QWmaegoCB8fX0N8iI7O5vNmzdXaF7IoW9g2LBh9O3bl8aNGxMdHc2UKVO4ePEizz//vLGbZtLi4+P55ZdfmDdvHk5OTvpzNC4uLtjb2xu5dabLycnpuvP4jo6OeHh4yPn923jttddo3rw5H374IU888QRbtmzh22+/5dtvvzV200xaXFwcH3zwAQEBAURERLBjxw4mT57MCy+8YOymmZzc3FwOHjyof5yamkpSUhLu7u4EBAQwdOhQ/u///o/Q0FCCgoJ477338Pf3p3v37hXXqAq7ntzMTJ06VQUEBCgbGxsVHR2tNm3aZOwmmTzghtP06dON3TSzI+VZd27+/Pmqbt26ytbWVoWHh6tvv/3W2E0yednZ2WrIkCEqICBA2dnZqZo1a6qRI0eq/Px8YzfN5KxcufKGv9f69u2rlNKVaL333nvKx8dH2draqpiYGJWSklKhbZLRs4QQQggT9sCfoxZCCCFMmQS1EEIIYcIkqIUQQggTJkEthBBCmDAJaiGEEMKESVALIYQQJkyCWgghhDBhEtRCCCGECZOgFkLcM41Gw9y5c43dDCEqJQlqIcxcv3790Gg0102dOnUydtOEEOVABuUQohLo1KkT06dPN5hna2trpNYIIcqT7FELUQnY2tri6+trMLm5uQG6w9LTpk2jc+fO2NvbU7NmTf744w+D5+/atYv27dtjb2+Ph4cHr7zyCrm5uQbr/PDDD0RERGBra4ufnx8DBw40WH7mzBl69OiBg4MDoaGhJCQk6JedP3+ePn364OXlhb29PaGhodf9YSGEuDEJaiEeAO+99x6PPfYYycnJ9OnThyeffJK9e/cCcPHiRTp27IibmxuJiYn8/vvvLFu2zCCIp02bRnx8PK+88gq7du0iISGBkJAQg9cYO3YsTzzxBDt37qRLly706dOHc+fO6V9/z549LF68mL179zJt2jQ8PT3v3xsghDmr0LG5hBAVrm/fvsrS0lI5OjoaTB988IFSSjcc6auvvmrwnKZNm6r+/fsrpZT69ttvlZubm8rNzdUvX7hwobKwsFAZGRlKKaX8/f3VyJEjb9oGQL377rv6x7m5uQpQixcvVkopFRcXp55//vny6bAQDxg5Ry1EJdCuXTumTZtmMM/d3V3/c7NmzQyWNWvWjKSkJAD27t1LZGQkjo6O+uUtWrRAq9WSkpKCRqMhLS2NmJiYW7ahfv36+p8dHR1xdnYmMzMTgP79+/PYY4+xfft2OnToQPfu3WnevHmZ+irEg0aCWohKwNHR8bpD0eXF3t7+jtaztrY2eKzRaNBqtQB07tyZo0ePsmjRIpYuXUpMTAzx8fFMmjSp3NsrRGUj56iFeABs2rTpuse1a9cGoHbt2iQnJ3Px4kX98vXr12NhYUFYWBhOTk7UqFGD5cuX31MbvLy86Nu3Lz///DNTpkzh22+/vaftCfGgkD1qISqB/Px8MjIyDOZZWVnpL9j6/fffady4MS1btmTmzJls2bKF//73vwD06dOH0aNH07dvX8aMGcPp06cZNGgQzz77LD4+PgCMGTOGV199FW9vbzp37kxOTg7r169n0KBBd9S+UaNG0ahRIyIiIsjPz2fBggX6PxSEELcmQS1EJfD333/j5+dnMC8sLIx9+/YBuiuyZ8+ezYABA/Dz82PWrFnUqVMHAAcHB5YsWcKQIUNo0qQJDg4OPPbYY0yePFm/rb59+5KXl8enn37KG2+8gaenJ7169brj9tnY2PD2229z5MgR7O3tadWqFbNnzy6HngtR+WmUUsrYjRBCVByNRsOcOXPo3r27sZsihCgDOUcthBBCmDAJaiGEEMKEyTlqISo5ObslhHmTPWohhBDChElQCyGEECZMgloIIYQwYRLUQgghhAmToBZCCCFMmAS1EEIIYcIkqIUQQggTJkEthBBCmDAJaiGEEMKE/T/Z7Y3fBqHhCAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot training and validation graphs (note this is for training with a small sample text; full pre-training to be implemented later using OpenAI and other open-source model weights.)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e94d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This cell moves the trained model to the CPU and sets it to evaluation mode (disabling dropout and other training-specific behaviors). It initializes the GPT-2 tokenizer, then generates 25 new tokens of text starting from the prompt \"Every effort moves you\" using the generate_text_simple function. The generated token IDs are decoded back to text and printed. This demonstrates how to use the trained model for inference (text generation) after training is complete.\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe0c7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "#This code block demonstrates how to generate the next token using a sample vocabulary and a set of logits. It applies softmax to convert logits to probabilities, then uses argmax to select the most likely token, and prints the corresponding word. This illustrates the standard greedy decoding approach.\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f5cfd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "047237d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9546f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1325dccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7f16a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e2581fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "#Top-K sampling\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "37e56259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2762a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcd3b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifiying the text generation function using temperature scaling and top-k sampling.\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0314815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\" Gisburn rather a--I felt nervous and uncertain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28399e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and saving the model weights in PyTorch\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdaa3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52fc02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "624e39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02b734",
   "metadata": {},
   "source": [
    "### Loading pretrained weights from OpenAI GPT-2 (a fully pretrained AI model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f662b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pretrained weights from OpenAI\n",
    "#pip install tensorflow tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70925bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a9e3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the download_and_load_gpt2 function\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "470c21ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 10.5kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 7.04MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 53.7kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:13<00:00, 36.6MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 1.53MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 4.10MiB/s]\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.44MiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\",\n",
    "                                          models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24ade974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_vocab: 50257\n",
      "n_ctx: 1024\n",
      "n_embd: 768\n",
      "n_head: 12\n",
      "n_layer: 12\n"
     ]
    }
   ],
   "source": [
    "# This code block prints out the configuration settings used for loading OpenAI GPT-2 pretrained weights.\n",
    "# It iterates through the 'settings' dictionary, which contains key model hyperparameters such as vocabulary size, context length, embedding dimension, number of attention heads, and number of transformer layers.\n",
    "# For each key-value pair in the dictionary, it prints the parameter name and its value.\n",
    "# This provides a clear summary of the model architecture and configuration, helping users verify that the correct settings have been loaded before proceeding with further model operations.\n",
    "\n",
    "for k, v in settings.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75d36e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "#This code block is designed to inspect the contents of the `params` dictionary, which contains the pretrained weights and parameters loaded from OpenAI's GPT-2 model using the `download_and_load_gpt2` function. The code utilizes Python's built-in `print` function to display the keys of the `params` dictionary, allowing users to see which tensors and configuration items have been loaded for further use in model initialization or analysis.\n",
    "\n",
    "#Based on the output, the 5 keys in the params dictionary from the OpenAI GPT-2 model are:\n",
    "    #'blocks' - The transformer blocks (all the layers containing self-attention and feed-forward networks)\n",
    "    #'b' - Bias parameters\n",
    "    #'g' - Gain parameters\n",
    "    #'wpe' - Word Position Embeddings (positional encodings)\n",
    "    #'wte' - Word Token Embeddings (vocabulary embeddings)\n",
    "\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "49e74659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96ba6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "#Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  #example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7006e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the OpenAI weights to the corresponding weight tensors in the GPTModel instance\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def assign (left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape},Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bae22",
   "metadata": {},
   "source": [
    "## Pretrained Weight Transfer Function for OpenAI GPT-2 Integration\n",
    "\n",
    "**Purpose:** Transfers OpenAI's original GPT-2 pretrained weights (TensorFlow format) to custom PyTorch GPT implementation, enabling immediate high-quality text generation without expensive retraining.\n",
    "\n",
    "**Libraries:** NumPy (array operations), PyTorch (tensor/parameter management), custom GPTModel architecture.\n",
    "\n",
    "**Weight Transfer Process:**\n",
    "\n",
    "**1. Embedding Layer Assignment**\n",
    "- Maps OpenAI's word position embeddings (`params['wpe']`) to model's positional embedding layer\n",
    "- Assigns word token embeddings (`params['wte']`) to token embedding layer\n",
    "- Establishes foundation for token-to-vector conversion\n",
    "\n",
    "**2. Multi-Head Attention Processing**\n",
    "- Iterates through each transformer block in the model architecture\n",
    "- Uses `np.split()` to separate OpenAI's concatenated QKV weight matrix into individual query, key, value matrices\n",
    "- Applies transpose operations (`.T`) to handle dimensional differences between TensorFlow and PyTorch formats\n",
    "- Assigns split matrices to corresponding attention layers with proper shape alignment\n",
    "\n",
    "**3. Attention Bias and Output Projection**\n",
    "- Splits combined QKV bias vectors into separate query, key, value biases\n",
    "- Maps attention output projection weights and biases from OpenAI's `c_proj` parameters\n",
    "- Ensures dimensional compatibility through transposition operations\n",
    "\n",
    "**4. Feed-Forward Network Transfer**\n",
    "- Assigns first linear layer weights/biases from `c_fc` parameters to custom model's feed-forward layer\n",
    "- Maps second linear layer weights/biases from `c_proj` to corresponding custom layer\n",
    "- Maintains two-layer feed-forward structure with proper weight transposition\n",
    "\n",
    "**5. Layer Normalization Parameters**\n",
    "- Transfers scale parameters (`g`) and shift parameters (`b`) from OpenAI's `ln_1` and `ln_2`\n",
    "- Maps to custom model's `norm1` and `norm2` layers for each transformer block\n",
    "- Ensures proper normalization behavior throughout network\n",
    "\n",
    "**6. Final Components and Output Head**\n",
    "- Assigns final layer normalization parameters from global OpenAI parameters\n",
    "- Sets up output head using weight tying (same embeddings for input/output projections)\n",
    "- Completes comprehensive parameter mapping for full model functionality\n",
    "\n",
    "**Result:** Every OpenAI GPT-2 parameter correctly positioned in custom PyTorch implementation, enabling immediate text generation with pretrained quality while maintaining architectural compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ccd047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b299a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use model to generate text using previous generate function.\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to\n",
    "    (device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
