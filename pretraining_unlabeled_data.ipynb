{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7db9585",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#228B22; font-weight:bold; margin-bottom:0.5em;\">Pretraining a GPT-Style Language Model from Scratch</h1>\n",
    "\n",
    "<p>\n",
    "<strong>Author:</strong> <span style=\"color:#FFD800;\">Anup Sharma</span><br>\n",
    "</p>\n",
    "\n",
    "<hr style=\"border-top: 2px solid #888888;\">\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><strong>Dataset</strong></td>\n",
    "    <td>Characters: <span style=\"color:#228B22;\">10,419</span></td>\n",
    "    <td>Tokens: <span style=\"color:#228B22;\">2,772</span></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Model</strong></td>\n",
    "    <td>Type: <span style=\"color:#228B22;\">GPT (Transformer)</span></td>\n",
    "    <td>Layers: <span style=\"color:#228B22;\">12</span></td>\n",
    "    <td>Heads: <span style=\"color:#228B22;\">12</span></td>\n",
    "    <td>Embedding Dim: <span style=\"color:#228B22;\">768</span></td>\n",
    "    <td>Context Length: <span style=\"color:#228B22;\">256</span></td>\n",
    "    <td>Epochs: <span style=\"color:#228B22;\">10</span></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<hr style=\"border-top: 2px solid #888888;\">\n",
    "\n",
    "<p style=\"font-size:1.1em; margin-bottom:0.5em;\"><strong>Workflow Overview:</strong></p>\n",
    "<p>\n",
    "  This notebook demonstrates how to pretrain a GPT-style language model from scratch on a small sample text dataset. The data is split into training and validation sets, and the model is trained for 10 epochs using the AdamW optimizer. At each epoch, the model processes data in batches, computes cross-entropy loss, updates its weights, periodically evaluates performance on both training and validation sets, and generates sample text to monitor progress. This workflow leverages all core features of the transformer architecture and provides a hands-on introduction to large language model pretraining.\n",
    "</p>\n",
    "\n",
    "<p style=\"color:#888;\">\n",
    "  <em>Run the cells below to inspect the dataset, model, and training process from scratch for educational purposes and future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629c0dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.5\n",
      "numpy version: 2.2.6\n",
      "tiktoken version: 0.11.0\n",
      "torch version: 2.8.0\n",
      "tensorflow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\" #For OpenAI's pretrained weights\n",
    "       ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414cd78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anup/gitProjects/language-models-psychiatry\n"
     ]
    }
   ],
   "source": [
    "#working directory\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bc60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gpt_from_scratch.GPTModel'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gpt_from_scratch import GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f22461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class GPTModel(nn.Module):\n",
       "    def __init__(self, cfg):\n",
       "        super().__init__()\n",
       "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
       "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
       "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
       "\n",
       "        self.trf_blocks = nn.Sequential(\n",
       "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
       "\n",
       "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
       "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
       "\n",
       "    def forward(self, in_idx):\n",
       "        batch_size, seq_len = in_idx.shape\n",
       "        tok_embeds = self.tok_emb(in_idx)\n",
       "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
       "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
       "        x = self.drop_emb(x)\n",
       "        x = self.trf_blocks(x)\n",
       "        x = self.final_norm(x)\n",
       "        logits = self.out_head(x)\n",
       "        return logits\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Inspect the GPT Model\n",
    "import inspect\n",
    "from gpt_from_scratch import GPTModel\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(GPTModel)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e26414f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config file that contains model parameters for GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d99a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  #Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcceefa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#View of the model architecture\n",
    "from IPython.display import display\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f792c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([256, 768])\n",
      "trf_blocks.0.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.0.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.0.norm1.scale torch.Size([768])\n",
      "trf_blocks.0.norm1.shift torch.Size([768])\n",
      "trf_blocks.0.norm2.scale torch.Size([768])\n",
      "trf_blocks.0.norm2.shift torch.Size([768])\n",
      "trf_blocks.1.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.1.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.1.norm1.scale torch.Size([768])\n",
      "trf_blocks.1.norm1.shift torch.Size([768])\n",
      "trf_blocks.1.norm2.scale torch.Size([768])\n",
      "trf_blocks.1.norm2.shift torch.Size([768])\n",
      "trf_blocks.2.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.2.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.2.norm1.scale torch.Size([768])\n",
      "trf_blocks.2.norm1.shift torch.Size([768])\n",
      "trf_blocks.2.norm2.scale torch.Size([768])\n",
      "trf_blocks.2.norm2.shift torch.Size([768])\n",
      "trf_blocks.3.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.3.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.3.norm1.scale torch.Size([768])\n",
      "trf_blocks.3.norm1.shift torch.Size([768])\n",
      "trf_blocks.3.norm2.scale torch.Size([768])\n",
      "trf_blocks.3.norm2.shift torch.Size([768])\n",
      "trf_blocks.4.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.4.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.4.norm1.scale torch.Size([768])\n",
      "trf_blocks.4.norm1.shift torch.Size([768])\n",
      "trf_blocks.4.norm2.scale torch.Size([768])\n",
      "trf_blocks.4.norm2.shift torch.Size([768])\n",
      "trf_blocks.5.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.5.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.5.norm1.scale torch.Size([768])\n",
      "trf_blocks.5.norm1.shift torch.Size([768])\n",
      "trf_blocks.5.norm2.scale torch.Size([768])\n",
      "trf_blocks.5.norm2.shift torch.Size([768])\n",
      "trf_blocks.6.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.6.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.6.norm1.scale torch.Size([768])\n",
      "trf_blocks.6.norm1.shift torch.Size([768])\n",
      "trf_blocks.6.norm2.scale torch.Size([768])\n",
      "trf_blocks.6.norm2.shift torch.Size([768])\n",
      "trf_blocks.7.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.7.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.7.norm1.scale torch.Size([768])\n",
      "trf_blocks.7.norm1.shift torch.Size([768])\n",
      "trf_blocks.7.norm2.scale torch.Size([768])\n",
      "trf_blocks.7.norm2.shift torch.Size([768])\n",
      "trf_blocks.8.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.8.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.8.norm1.scale torch.Size([768])\n",
      "trf_blocks.8.norm1.shift torch.Size([768])\n",
      "trf_blocks.8.norm2.scale torch.Size([768])\n",
      "trf_blocks.8.norm2.shift torch.Size([768])\n",
      "trf_blocks.9.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.9.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.9.norm1.scale torch.Size([768])\n",
      "trf_blocks.9.norm1.shift torch.Size([768])\n",
      "trf_blocks.9.norm2.scale torch.Size([768])\n",
      "trf_blocks.9.norm2.shift torch.Size([768])\n",
      "trf_blocks.10.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.10.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.10.norm1.scale torch.Size([768])\n",
      "trf_blocks.10.norm1.shift torch.Size([768])\n",
      "trf_blocks.10.norm2.scale torch.Size([768])\n",
      "trf_blocks.10.norm2.shift torch.Size([768])\n",
      "trf_blocks.11.att.W_query.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_key.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.W_value.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.weight torch.Size([768, 768])\n",
      "trf_blocks.11.att.out_proj.bias torch.Size([768])\n",
      "trf_blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.11.norm1.scale torch.Size([768])\n",
      "trf_blocks.11.norm1.shift torch.Size([768])\n",
      "trf_blocks.11.norm2.scale torch.Size([768])\n",
      "trf_blocks.11.norm2.shift torch.Size([768])\n",
      "final_norm.scale torch.Size([768])\n",
      "final_norm.shift torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n",
      "\n",
      "number of parameter layers: 161\n"
     ]
    }
   ],
   "source": [
    "#View of the model's parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "print(f\"\\nnumber of parameter layers: {len(list(model.named_parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f395407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
       "    # idx is (B, T) array of indices in the current context\n",
       "    for _ in range(max_new_tokens):\n",
       "\n",
       "        # Crop current context if it exceeds the supported context size\n",
       "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
       "        # then only the last 5 tokens are used as context\n",
       "        idx_cond = idx[:, -context_size:]\n",
       "\n",
       "        # Get the predictions\n",
       "        with torch.no_grad():\n",
       "            logits = model(idx_cond)\n",
       "\n",
       "        # Focus only on the last time step\n",
       "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
       "        logits = logits[:, -1, :]\n",
       "\n",
       "        # Get the idx of the vocab entry with the highest logits value\n",
       "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
       "\n",
       "        # Append sampled index to the running sequence\n",
       "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
       "\n",
       "    return idx\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This block of code follows after viewing the GPT-2 model architecture and weights to transition from model inspection to practical usage. It ensures the tokenizer is available, adds the current directory to the Python path so local modules can be imported, and then imports the generate_text_simple function, which is essential for generating new text with the model.\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Add the current notebook's directory to sys.path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "#Import generate_text_simple function from gpt_from_scratch.py\n",
    "from gpt_from_scratch import generate_text_simple\n",
    "\n",
    "#Visualize the function\n",
    "import inspect\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```python\\n{inspect.getsource(generate_text_simple)}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0d0064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#This block demonstrates how to use the loaded GPT-2 model for text generation. It defines helper functions to convert text to token IDs and back (i.e. encode and decode) and then uses the generate_text_simple function to generate next tokens from the model. Finally, it decodes and prints the generated output as readable text, showing the practical application of th emodel for generating language.\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\",token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines two small batch of tokenized inputs and their corresponding target sequences. The targets are the next tokens for each input, which is is standard in language modeling.\n",
    "\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "#Decodes a sequence of token IDS from a previous generation step into human readable text using the tokenizer.\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307844e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "#Passes the inputs batch through the GPT model to get raw output scores (logits) for each token position and vocabulary entry. Applies softmax to convert logist to probabilities over the vocabulary for each token position.\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c321b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "#For each token position, selects the token ID with the highest probability. token_ids now contains the model's predicted next token for each position in the batch.\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae211eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "#Decodes the first row of targets and model's predicted token_ids back to text for easy comparison. Allows one to see how well the model's predictions (in this untrained model) match the expected next tokens.\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a5ec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "#Extracts probabilities assigned by the model to the correct or next tokens for each position in both input sequences\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Computes the log-probabilities of the correct tokens and then averages them. The average log-probability is a standard metric for model performance (higher is better) and is the basis for cross entropy calculations.\n",
    "\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b55869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Negates the average log-probability to convert it into a loss (since optimization frameworks minimize loss. This is the cross-entropy value, the standard function for language modeling.\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape of the inputs and outputs.\n",
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a989ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Flatten tensors by combining them over the batch dimension.\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0fa7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "#Using PyTorch built-in cross-entropy function to compute the loss in a numerically stable and efficient way.\n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "#Calculates perplexity, a commonly used metric in language modeling that is the exponentiated cross-entrophy loss. (i.e. lower perplexity means the model is more confident and accurate in its predictions)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac100f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and load a small text dataset for training and validation\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "148e4333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "#Checking the text\n",
    "print(text_data[:99])\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "#Text length in terms of characters and tokens\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8134091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset into a training and validation set and use the data loaders to prepare the batches for language model training\n",
    "\n",
    "from gpt_from_scratch import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2970816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for errors\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c63fcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"Validation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3fee5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens are: 4608 + 512 or 5120\n"
     ]
    }
   ],
   "source": [
    "#Confirming token sizes are in the expected range.\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(f\"All tokens are: {train_tokens} + {val_tokens} or\",  train_tokens + val_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4c188ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define utility functions to compute loss for a batch or an entire DataLoader\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e0074b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "Training loss: 0.10826531797647476\n",
      "Validation loss: 6.802339553833008\n"
     ]
    }
   ],
   "source": [
    "#Moves the model to the appropriate device (CPU/GPU/MPS), computes and prints the intitial training and validation losses. This establishes a baseline before training.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): #Disable gradient tracking not yet training\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "657c1cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients from prior batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculates the gradients\n",
    "            optimizer.step()  # Update model weights\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "        # Optional evaluation step\n",
    "        if global_step % eval_freq == 0:\n",
    "            train_loss, val_loss = evaluate_model(\n",
    "                model, train_loader, val_loader, device, eval_iter)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            track_tokens_seen.append(tokens_seen)\n",
    "            print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                  f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ceacfd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000008): Train loss 7.310, Val loss 7.465\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Every effort moves you.                                                 \n",
      "Every effort moves you.                           \" I had a I had\" I had the picture had to the picture and I had to me--I\n",
      "Ep 5 (Step 000044): Train loss 3.551, Val loss 6.204\n",
      "Every effort moves you know to                                                \n",
      "Every effort moves you know it to see a little to have to the picture--I looked of that he had been--I had to see it was to see the donkey, and in the picture at my elbow and in the donkey, and down the room, with the\n",
      "Every effort moves you know it was not that the picture for a smile that, I was not--so it was no--the the fact, with a little a smile behind his pictures--I had been his own he had the donkey.      \n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.                  He placed them at my elbow and as his pictures? a and were, and in his\n",
      "Ep 9 (Step 000080): Train loss 0.912, Val loss 6.277\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.     \"I didn't you know, and threw back the head to look up at the honour being _mine_--because he's the first\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.26 minutes.\n"
     ]
    }
   ],
   "source": [
    "#Now train model with the execution time\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = training_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=4, eval_iter=4,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time)/ 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f604475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6e94d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This cell moves the trained model to the CPU and sets it to evaluation mode (disabling dropout and other training-specific behaviors). It initializes the GPT-2 tokenizer, then generates 25 new tokens of text starting from the prompt \"Every effort moves you\" using the generate_text_simple function. The generated token IDs are decoded back to text and printed. This demonstrates how to use the trained model for inference (text generation) after training is complete.\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fe0c7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "#This code block demonstrates how to generate the next token using a sample vocabulary and a set of logits. It applies softmax to convert logits to probabilities, then uses argmax to select the most likely token, and prints the corresponding word. This illustrates the standard greedy decoding approach.\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f5cfd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "047237d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9546f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1325dccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f7f16a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e2581fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "#Top-k sampling\n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "37e56259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d2762a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOdifiying the text generation function using temperature scaling and top-k sampling.\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0314815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\" Gisburn rather a--I felt nervous and uncertain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "28399e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and saving the model weights in PyTorch\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cdaa3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "52fc02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "624e39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "70925bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "#Loading pretrained weights from OpenAI\n",
    "\n",
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
