{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd28c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import matplotlib\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94046e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "torch version: 2.5.1\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "#Versions of each package\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5de98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of 124 million parameter GPT-2 model:\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    #Vocabulary size\n",
    "    \"context_length\": 1024, #Context length\n",
    "    \"emb_dim\": 768,         #Embedding dimension\n",
    "    \"n_heads\": 12,          #Number of attention heads\n",
    "    \"n_layers\": 12,         #Number of layers\n",
    "    \"drop_rate\": 0.1,       #Dropout rate\n",
    "    \"qkv_bias\": False       #Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3965d7",
   "metadata": {},
   "source": [
    "**Heading: Building a Minimal GPT-like Model with PyTorch**\n",
    "\n",
    "This code demonstrates how to construct a simplified version of a GPT (Generative Pretrained Transformer) model using PyTorch. The script is structured to show the main architectural components of a transformer-based language model, using placeholder classes for the transformer block and layer normalization to focus on the overall structure. Here’s a summary what each part does:\n",
    "\n",
    "1. **Importing Libraries:**  \n",
    "   - `torch` and `torch.nn` are imported to provide tensor operations and neural network layers, which are essential for building and training deep learning models.\n",
    "\n",
    "2. **Defining the DummyGPTModel Class:**  \n",
    "   - Inherits from `nn.Module`, making it compatible with PyTorch’s model API.\n",
    "   - The `__init__` method initializes the model’s layers:\n",
    "     - `self.tok_emb`: A token embedding layer that converts input token indices into dense vectors of size `emb_dim`.\n",
    "     - `self.pos_emb`: A positional embedding layer that encodes the position of each token in the sequence, also as vectors of size `emb_dim`.\n",
    "     - `self.drop_emb`: A dropout layer for regularization, randomly zeroing some elements to prevent overfitting.\n",
    "     - `self.trf_blocks`: A stack of placeholder transformer blocks, repeated `n_layers` times, using `nn.Sequential` to apply them in order.\n",
    "     - `self.final_norm`: A placeholder for the final layer normalization, which would typically normalize the output before the final prediction.\n",
    "     - `self.out_head`: A linear layer that projects the final hidden states back to the vocabulary size, producing logits for each token.\n",
    "   - The `forward` method defines the data flow:\n",
    "     - Takes input token indices, computes token and positional embeddings, sums them, applies dropout, passes through the transformer blocks, applies final normalization, and projects to output logits.\n",
    "\n",
    "3. **Defining DummyTransformerBlock:**  \n",
    "   - A placeholder class that mimics a transformer block’s interface but simply returns its input unchanged. In a full model, this would contain attention and feedforward sublayers.\n",
    "\n",
    "4. **Defining DummyLayerNorm:**  \n",
    "   - Another placeholder that mimics layer normalization but returns its input unchanged. In a real model, this would normalize activations to stabilize training.\n",
    "\n",
    "**Key Steps in the Model:**\n",
    "- Embedding input tokens and their positions.\n",
    "- Applying dropout for regularization.\n",
    "- Passing data through a stack of transformer blocks (here, placeholders).\n",
    "- Applying layer normalization (placeholder).\n",
    "- Projecting to vocabulary logits for language modeling.\n",
    "\n",
    "**Inputs and Parameters:**\n",
    "- The model expects a configuration dictionary (`cfg`) specifying vocabulary size, embedding dimension, context length, number of layers, dropout rate, etc.\n",
    "- The `forward` method expects a tensor of token indices as input.\n",
    "\n",
    "This structure provides a clear template for building a transformer-based language model in PyTorch, with placeholders that can be replaced by full implementations for attention, feedforward, and normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10401dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    #Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range (cfg [\"n_layers\"])])\n",
    "\n",
    "    #Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602737b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd54bf",
   "metadata": {},
   "source": [
    "\n",
    "**Heading: Running a Dummy GPT Model on Tokenized Input Using PyTorch**\n",
    "\n",
    "This code block demonstrates how to initialize and run a minimal GPT-like model using PyTorch, with placeholder components for the transformer and normalization layers. The purpose is to verify the data flow and output shape of the model using a batch of tokenized sentences. The code uses the PyTorch library for model definition and tensor operations. Here’s a breakdown of each step:\n",
    "\n",
    "1. `torch.manual_seed(123)` sets the random seed for PyTorch to ensure reproducibility, so that model weights and any random operations yield the same results each time the code is run.\n",
    "2. `model = DummyGPTModel(GPT_CONFIG_124M)` creates an instance of the dummy GPT model using a configuration dictionary that specifies model parameters such as vocabulary size, embedding dimension, number of layers, and dropout rate. The model uses placeholder transformer and normalization blocks, so it does not perform real attention or normalization.\n",
    "3. `logits = model(batch)` passes a batch of tokenized input sentences (prepared earlier) through the model. The model processes the input through embedding, dropout, and placeholder layers, then outputs logits or a tensor of raw, unnormalized predictions for each token position and vocabulary entry.\n",
    "4. `print(\"Output shape:\", logits.shape)` prints the shape of the output tensor, which should match `[batch_size, sequence_length, vocab_size]`, confirming that the model produces outputs of the expected dimensions.\n",
    "5. `print(logits)` prints the actual logits tensor, showing the raw prediction values for each token in the input batch. Since the model is untrained and uses dummy components, these values are random.\n",
    "\n",
    "**Summary of Key Steps:**\n",
    "- Set a random seed for reproducibility.\n",
    "- Instantiate a dummy GPT model with specified architecture.\n",
    "- Run a batch of tokenized sentences through the model to obtain output logits.\n",
    "- Print the shape and values of the output tensor to verify correct data flow and output structure.\n",
    "\n",
    "This block is useful for confirming that the model architecture and data pipeline are functioning as intended before implementing more complex transformer logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df19cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a18a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Output shape torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "#create 2 training examples with 5 dimensions or features\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear (5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "print(\"Output shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
