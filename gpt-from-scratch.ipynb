{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd28c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import matplotlib\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94046e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "torch version: 2.5.1\n",
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "#Versions of each package\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5de98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of 124 million parameter GPT-2 model:\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    #Vocabulary size\n",
    "    \"context_length\": 1024, #Context length\n",
    "    \"emb_dim\": 768,         #Embedding dimension\n",
    "    \"n_heads\": 12,          #Number of attention heads\n",
    "    \"n_layers\": 12,         #Number of layers\n",
    "    \"drop_rate\": 0.1,       #Dropout rate\n",
    "    \"qkv_bias\": False       #Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3965d7",
   "metadata": {},
   "source": [
    "**Heading: Building a Minimal GPT-like Model with PyTorch**\n",
    "\n",
    "This code demonstrates how to construct a simplified version of a GPT (Generative Pretrained Transformer) model using PyTorch. The script is structured to show the main architectural components of a transformer-based language model, using placeholder classes for the transformer block and layer normalization to focus on the overall structure. Here’s a summary what each part does:\n",
    "\n",
    "1. **Importing Libraries:**  \n",
    "   - `torch` and `torch.nn` are imported to provide tensor operations and neural network layers, which are essential for building and training deep learning models.\n",
    "\n",
    "2. **Defining the DummyGPTModel Class:**  \n",
    "   - Inherits from `nn.Module`, making it compatible with PyTorch’s model API.\n",
    "   - The `__init__` method initializes the model’s layers:\n",
    "     - `self.tok_emb`: A token embedding layer that converts input token indices into dense vectors of size `emb_dim`.\n",
    "     - `self.pos_emb`: A positional embedding layer that encodes the position of each token in the sequence, also as vectors of size `emb_dim`.\n",
    "     - `self.drop_emb`: A dropout layer for regularization, randomly zeroing some elements to prevent overfitting.\n",
    "     - `self.trf_blocks`: A stack of placeholder transformer blocks, repeated `n_layers` times, using `nn.Sequential` to apply them in order.\n",
    "     - `self.final_norm`: A placeholder for the final layer normalization, which would typically normalize the output before the final prediction.\n",
    "     - `self.out_head`: A linear layer that projects the final hidden states back to the vocabulary size, producing logits for each token.\n",
    "   - The `forward` method defines the data flow:\n",
    "     - Takes input token indices, computes token and positional embeddings, sums them, applies dropout, passes through the transformer blocks, applies final normalization, and projects to output logits.\n",
    "\n",
    "3. **Defining DummyTransformerBlock:**  \n",
    "   - A placeholder class that mimics a transformer block’s interface but simply returns its input unchanged. In a full model, this would contain attention and feedforward sublayers.\n",
    "\n",
    "4. **Defining DummyLayerNorm:**  \n",
    "   - Another placeholder that mimics layer normalization but returns its input unchanged. In a real model, this would normalize activations to stabilize training.\n",
    "\n",
    "**Key Steps in the Model:**\n",
    "- Embedding input tokens and their positions.\n",
    "- Applying dropout for regularization.\n",
    "- Passing data through a stack of transformer blocks (here, placeholders).\n",
    "- Applying layer normalization (placeholder).\n",
    "- Projecting to vocabulary logits for language modeling.\n",
    "\n",
    "**Inputs and Parameters:**\n",
    "- The model expects a configuration dictionary (`cfg`) specifying vocabulary size, embedding dimension, context length, number of layers, dropout rate, etc.\n",
    "- The `forward` method expects a tensor of token indices as input.\n",
    "\n",
    "This structure provides a clear template for building a transformer-based language model in PyTorch, with placeholders that can be replaced by full implementations for attention, feedforward, and normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10401dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    #Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range (cfg [\"n_layers\"])])\n",
    "\n",
    "    #Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602737b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd54bf",
   "metadata": {},
   "source": [
    "\n",
    "**Heading: Running a Dummy GPT Model on Tokenized Input Using PyTorch**\n",
    "\n",
    "This code block demonstrates how to initialize and run a minimal GPT-like model using PyTorch, with placeholder components for the transformer and normalization layers. The purpose is to verify the data flow and output shape of the model using a batch of tokenized sentences. The code uses the PyTorch library for model definition and tensor operations. Here’s a breakdown of each step:\n",
    "\n",
    "1. `torch.manual_seed(123)` sets the random seed for PyTorch to ensure reproducibility, so that model weights and any random operations yield the same results each time the code is run.\n",
    "2. `model = DummyGPTModel(GPT_CONFIG_124M)` creates an instance of the dummy GPT model using a configuration dictionary that specifies model parameters such as vocabulary size, embedding dimension, number of layers, and dropout rate. The model uses placeholder transformer and normalization blocks, so it does not perform real attention or normalization.\n",
    "3. `logits = model(batch)` passes a batch of tokenized input sentences (prepared earlier) through the model. The model processes the input through embedding, dropout, and placeholder layers, then outputs logits or a tensor of raw, unnormalized predictions for each token position and vocabulary entry.\n",
    "4. `print(\"Output shape:\", logits.shape)` prints the shape of the output tensor, which should match `[batch_size, sequence_length, vocab_size]`, confirming that the model produces outputs of the expected dimensions.\n",
    "5. `print(logits)` prints the actual logits tensor, showing the raw prediction values for each token in the input batch. Since the model is untrained and uses dummy components, these values are random.\n",
    "\n",
    "**Summary of Key Steps:**\n",
    "- Set a random seed for reproducibility.\n",
    "- Instantiate a dummy GPT model with specified architecture.\n",
    "- Run a batch of tokenized sentences through the model to obtain output logits.\n",
    "- Print the shape and values of the output tensor to verify correct data flow and output structure.\n",
    "\n",
    "This block is useful for confirming that the model architecture and data pipeline are functioning as intended before implementing more complex transformer logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df19cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Output shape torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "#Inputing a small sample through a simple neural network layer\n",
    "torch.manual_seed(123)\n",
    "\n",
    "#create 2 training examples with 5 dimensions or features\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear (5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)\n",
    "print(\"Output shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0cb2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Calculate the mean and variance for each of the two inputs\n",
    "\n",
    "#The normalization is applied to each of the two inputs (rows) independently; using dim=-1 applies the calculation across the last dimension (in this case, the feature dimension) instead of the row\n",
    "\n",
    "\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84dc2d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean: tensor([[-2.8908e-06],\n",
      "        [-3.5763e-07]], grad_fn=<MeanBackward1>)\n",
      "Variance tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Subtracting the mean and dividing by the square-root of the variance (standard deviation) centers the inputs to have a mean of 0 and a variance of 1 across the column (feature) dimension:\n",
    "\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f70d915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[    -0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#To improve the readability, disabling PyTorch scientific notation\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a73fc",
   "metadata": {},
   "source": [
    "---\n",
    "**Purpose:**  \n",
    "This code implements a custom Layer Normalization (LayerNorm) module in PyTorch, which normalizes the activations of each input sample across its feature dimension. LayerNorm helps stabilize and speed up neural network training by ensuring each input has zero mean and unit variance, with learnable scaling and shifting.\n",
    "\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "1. **Class Definition:**  \n",
    "   - `class LayerNorm(nn.Module):`  \n",
    "     Defines a new neural network module called `LayerNorm` that inherits from PyTorch’s `nn.Module`.\n",
    "\n",
    "2. **Initialization (`__init__`):**  \n",
    "   - `def __init__(self, emb_dim):`  \n",
    "     The constructor takes `emb_dim` (embedding dimension or number of features) as input.\n",
    "   - `super().__init__()`  \n",
    "     Calls the parent class constructor.\n",
    "   - `self.eps = 1e-5`  \n",
    "     Sets a small epsilon value to avoid division by zero during normalization.\n",
    "   - `self.scale = nn.Parameter(torch.ones(emb_dim))`  \n",
    "     Initializes a learnable scaling parameter (gamma), starting as a vector of ones (one per feature).\n",
    "   - `self.shift = nn.Parameter(torch.zeroes(emb_dim))`  \n",
    "     Initializes a learnable shifting parameter (beta), starting as a vector of zeros (one per feature).  \n",
    "\n",
    "3. **Forward Pass (`forward`):**  \n",
    "   - `def forward(self, x):`  \n",
    "     Defines how the input tensor `x` is processed.\n",
    "   - `mean = x.mean(dim=-1, keepdim=True)`  \n",
    "     Computes the mean across the last dimension (features) for each input sample.\n",
    "   - `var = x.vari(dim=-1, keepdim=True, unbiased=False)`  \n",
    "     Computes the variance across the last dimension.  \n",
    "     \n",
    "   - `norm_x = (x - mean) / torch.sqrt(var + self.eps)`  \n",
    "     Normalizes the input: subtracts the mean and divides by the standard deviation (variance + epsilon).\n",
    "   - `return self.scale * norm_x + self.shift`  \n",
    "     Applies the learnable scale and shift to the normalized input, allowing the model to undo normalization if needed.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "This custom `LayerNorm` class normalizes each input sample’s features to have zero mean and unit variance, then applies learnable scaling and shifting. This is a standard technique in modern neural networks, especially transformers, to improve training stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb6628e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating steps to implement a LayerNorm class using mean, variance, scale (ones), shift (zeroes) as trainable parameters. EPS is a small value added before computing to avoid division by zero.\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a5ae924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "print(out_ln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "611cfe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a feed forward network with GELU (Gaussian Error Linear Unit) activation using adopted approximation function.\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92229fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "#Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4482535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
